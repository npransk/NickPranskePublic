{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O480WZvbQedp"
      },
      "source": [
        "# Yahtzee Deep Q-Learning Training\n",
        "\n",
        "This notebook implements a Dueling DQN agent to play Yahtzee using prioritized experience replay."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga4z3AEfQedq"
      },
      "source": [
        "## Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKqu4gJrEr83",
        "outputId": "548386f9-9e67-40ad-b763-c2ecf29060f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "save_path = '/content/drive'\n",
        "checkpoint_path = '/content/drive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1Q37aBLQedq",
        "outputId": "59328d7b-563f-4675-e1e2-e71bfb9a1ed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA L4\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "import os\n",
        "\n",
        "\"\"\"\n",
        "COMPUTE DEVICE SETUP:\n",
        "This whole program would not have been possible if it weren't for cloud-based GPUs.\n",
        "More details in the READMEs, but I ran this on an NVIDIA L4 GPU via a Google Colab notebook.\n",
        "\"\"\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU: Warning! No GPU Detected\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Define random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "# Define Yahtzee categories\n",
        "CATEGORIES = [\n",
        "    'ones', 'twos', 'threes', 'fours', 'fives', 'sixes',\n",
        "    'three_of_kind', 'four_of_kind', 'full_house',\n",
        "    'small_straight', 'large_straight', 'yahtzee', 'chance'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9RIJ9TwQedq"
      },
      "source": [
        "## Yahtzee Game Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teU-H9FoQedr"
      },
      "outputs": [],
      "source": [
        "# Create Yahtzee gameplay env\n",
        "class YahtzeeGame:\n",
        "\n",
        "    # The next few functions are to reset the state, this first one is to automatically do that at the start\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.dice = np.zeros(5, dtype=int)\n",
        "        self.scorecard = {cat: None for cat in CATEGORIES}\n",
        "        self.roll_count = 0\n",
        "        self.turn = 0\n",
        "        self.total_score = 0\n",
        "        self.yahtzee_bonus_count = 0\n",
        "        self.first_roll_of_turn = True\n",
        "        self.game_log = []  # Track for analysis\n",
        "        return self.get_state()\n",
        "\n",
        "    # Define the action to roll the dice; this is done up to 3 times per turn\n",
        "    def roll_dice(self, keep_mask=None):\n",
        "\n",
        "        if keep_mask is None:\n",
        "            keep_mask = [0, 0, 0, 0, 0]\n",
        "\n",
        "        for i in range(5):\n",
        "            if not keep_mask[i]:\n",
        "                self.dice[i] = random.randint(1, 6)\n",
        "\n",
        "        self.roll_count += 1\n",
        "        self.first_roll_of_turn = False\n",
        "        self.game_log.append(('roll', keep_mask, self.dice.copy()))\n",
        "        return self.dice.copy()\n",
        "\n",
        "    # In order for Q-learning to work, we must calculate what the current dice would score in a given category\n",
        "    def calculate_score(self, category):\n",
        "        dice = self.dice\n",
        "        counts = np.bincount(dice, minlength=7)[1:]\n",
        "\n",
        "        # Define scoring rules for each category\n",
        "        if category == 'ones':\n",
        "            return np.sum(dice == 1)\n",
        "        elif category == 'twos':\n",
        "            return np.sum(dice == 2) * 2\n",
        "        elif category == 'threes':\n",
        "            return np.sum(dice == 3) * 3\n",
        "        elif category == 'fours':\n",
        "            return np.sum(dice == 4) * 4\n",
        "        elif category == 'fives':\n",
        "            return np.sum(dice == 5) * 5\n",
        "        elif category == 'sixes':\n",
        "            return np.sum(dice == 6) * 6\n",
        "        elif category == 'three_of_kind':\n",
        "            return np.sum(dice) if np.max(counts) >= 3 else 0\n",
        "        elif category == 'four_of_kind':\n",
        "            return np.sum(dice) if np.max(counts) >= 4 else 0\n",
        "        elif category == 'full_house':\n",
        "            return 25 if sorted(counts[counts > 0]) == [2, 3] else 0\n",
        "        elif category == 'small_straight':\n",
        "            dice_set = set(dice)\n",
        "            straights = [{1,2,3,4}, {2,3,4,5}, {3,4,5,6}]\n",
        "            return 30 if any(s.issubset(dice_set) for s in straights) else 0\n",
        "        elif category == 'large_straight':\n",
        "            return 40 if set(dice) in [{1,2,3,4,5}, {2,3,4,5,6}] else 0\n",
        "        elif category == 'yahtzee':\n",
        "            return 50 if np.max(counts) == 5 else 0\n",
        "        elif category == 'chance':\n",
        "            return np.sum(dice)\n",
        "        return 0\n",
        "\n",
        "    # Using the above parameters for each category, we can now score the current dice in a given category\n",
        "    # Returns -1 if invalid move\n",
        "    def score_category(self, category):\n",
        "        if self.scorecard[category] is not None:\n",
        "            return -1  # Category already used\n",
        "        if self.first_roll_of_turn:\n",
        "            return -1  # Must roll before scoring\n",
        "\n",
        "        score = self.calculate_score(category)\n",
        "\n",
        "        # Yahtzee bonus: +100 for each additional Yahtzee after first\n",
        "        is_yahtzee = np.max(np.bincount(self.dice, minlength=7)[1:]) == 5\n",
        "        if is_yahtzee and self.scorecard['yahtzee'] is not None and self.scorecard['yahtzee'] > 0:\n",
        "            score += 100\n",
        "            self.yahtzee_bonus_count += 1\n",
        "\n",
        "        self.scorecard[category] = score\n",
        "        self.total_score += score\n",
        "        self.game_log.append(('score', category, score))\n",
        "\n",
        "        # Upper section bonus at game end (+35 if 63+ points)\n",
        "        if self.is_game_over():\n",
        "            upper_score = sum(self.scorecard[cat] for cat in CATEGORIES[:6] if self.scorecard[cat] is not None)\n",
        "            if upper_score >= 63:\n",
        "                self.total_score += 35\n",
        "                self.game_log.append(('bonus', 'upper', 35))\n",
        "\n",
        "        # Reset for next turn\n",
        "        self.turn += 1\n",
        "        self.roll_count = 0\n",
        "        self.first_roll_of_turn = True\n",
        "\n",
        "        return score\n",
        "\n",
        "    def is_game_over(self):\n",
        "        return all(score is not None for score in self.scorecard.values())\n",
        "\n",
        "    def get_state(self):\n",
        "        # Get current game state as feature vector.\n",
        "\n",
        "        # Dice one-hot encoding (see READMEs for explanation) - basically just a way to turn categories into numbers\n",
        "        dice_onehot = np.zeros(30)\n",
        "        for i, die in enumerate(self.dice):\n",
        "            if die > 0:\n",
        "                dice_onehot[i * 6 + die - 1] = 1\n",
        "\n",
        "        # Scorecard status\n",
        "        scorecard_filled = np.array([1.0 if self.scorecard[cat] is not None else 0.0 for cat in CATEGORIES])\n",
        "\n",
        "        \"\"\"\n",
        "        I like this next part so I want to explain it here:\n",
        "        The program will create an array of placeholders for each category, then for each category\n",
        "        that is still available it will calculate what the current roll would score. So let's sat that\n",
        "        you roll a [2,2,5,5,6]. It will calculate the categories as this:\n",
        "        [0, 4, 0, 0, 10, 6, 0, 0, 0, 0, 0, 0, 20]\n",
        "\n",
        "        But then it will normalize it to the highest normal score in Yahtzee (50):\n",
        "        [0, 0.08, 0, 0, 0.2, 0.12, 0, 0, 0, 0, 0, 0, 0.4]\n",
        "\n",
        "        and use that to figure out which categories are most valuable to go for with the current roll!\n",
        "        \"\"\"\n",
        "\n",
        "        # Potential scores (normalized)\n",
        "        potential_scores = np.zeros(13) # placeholders for each category\n",
        "        if not self.first_roll_of_turn: # Don't need to calculate if no roll yet\n",
        "            for i, cat in enumerate(CATEGORIES):\n",
        "                if self.scorecard[cat] is None: # if category is still available\n",
        "                    potential_scores[i] = self.calculate_score(cat) / 50.0 # Normalize to between 0 and 1\n",
        "\n",
        "        # Upper section progress (critical for 63-point bonus strategy)\n",
        "        upper_scores = np.array([\n",
        "            (self.scorecard[cat] if self.scorecard[cat] is not None else 0) / 18.0\n",
        "            for cat in CATEGORIES[:6]\n",
        "        ])\n",
        "\n",
        "        # Turn and roll info\n",
        "        roll_count = np.array([self.roll_count / 3.0])\n",
        "        turn_progress = np.array([self.turn / 13.0])\n",
        "        upper_sum = sum(self.scorecard[cat] if self.scorecard[cat] is not None else 0\n",
        "                       for cat in CATEGORIES[:6])\n",
        "        upper_progress = np.array([min(upper_sum / 63.0, 1.5)])  # Can exceed 1.0\n",
        "        has_rolled = np.array([0.0 if self.first_roll_of_turn else 1.0])\n",
        "        score_normalized = np.array([self.total_score / 400.0])\n",
        "        yahtzee_bonuses = np.array([self.yahtzee_bonus_count / 3.0])\n",
        "\n",
        "        # return the features the model is going to evaulate\n",
        "        return np.concatenate([\n",
        "            dice_onehot,           # 30\n",
        "            scorecard_filled,      # 13\n",
        "            potential_scores,      # 13\n",
        "            upper_scores,          # 6\n",
        "            roll_count,            # 1\n",
        "            turn_progress,         # 1\n",
        "            upper_progress,        # 1\n",
        "            has_rolled,            # 1\n",
        "            score_normalized,      # 1\n",
        "            yahtzee_bonuses        # 1\n",
        "        ])\n",
        "\n",
        "    def get_valid_actions(self):\n",
        "        #n Get all valid actions from current state\n",
        "        actions = []\n",
        "\n",
        "        # Must roll at start of turn\n",
        "        if self.first_roll_of_turn:\n",
        "            actions.append(('roll', tuple([0, 0, 0, 0, 0])))\n",
        "            return actions\n",
        "\n",
        "        # Can roll if under 3 rolls this turn\n",
        "        if self.roll_count < 3:\n",
        "            for i in range(32):\n",
        "                keep_mask = tuple((i >> j) & 1 for j in range(5))\n",
        "                actions.append(('roll', keep_mask))\n",
        "\n",
        "        # Can score in any available category\n",
        "        for cat in CATEGORIES:\n",
        "            if self.scorecard[cat] is None:\n",
        "                actions.append(('score', cat))\n",
        "\n",
        "        return actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5rEBl3SQeds"
      },
      "source": [
        "## Neural Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hURPgGYtQeds"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This is where most of the magic happens. An explanation of Dueling DQN is in the READMEs\n",
        "\"\"\"\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=512):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "\n",
        "        self.feature = nn.Sequential( # shared network\n",
        "            nn.Linear(state_size, hidden_size), # fully connected layers\n",
        "            nn.ReLU(), # best activation function, according to my professors in grad school\n",
        "            nn.LayerNorm(hidden_size), # the internet told me to add this\n",
        "            nn.Dropout(0.1), # apparently reduces overfitting\n",
        "            ### Repeat for hidden layers\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.Dropout(0.1),\n",
        "        )\n",
        "\n",
        "        # Value stream: estimates state value\n",
        "        # V(s) = how good the state is\n",
        "        self.value = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 2, 1)\n",
        "        )\n",
        "\n",
        "        # Advantage stream: estimates action advantages\n",
        "        # This is the benefit of Dueling DQN over Q-learning - it separates state value from action advantage\n",
        "        self.advantage = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 2, action_size)\n",
        "        )\n",
        "\n",
        "    # Dueling architecture combines streams to get Q-values\n",
        "    def forward(self, x):\n",
        "        features = self.feature(x)\n",
        "        value = self.value(features)\n",
        "        advantage = self.advantage(features)\n",
        "\n",
        "        # Combine: Q = V + (A - mean(A))\n",
        "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "        return q_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDhcw3_rQeds"
      },
      "source": [
        "## Prioritized Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciyWcW1oQeds"
      },
      "outputs": [],
      "source": [
        "class PrioritizedReplayBuffer:\n",
        "\n",
        "    # Initialize buffer with memory capacity and prioritization alpha (how strongly to priorize high-error samples)\n",
        "    def __init__(self, capacity=300000, alpha=0.7): # High alpha so it can take risks to hopefully beat mathematical strategies\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.buffer = []\n",
        "        self.priorities = []\n",
        "        self.position = 0\n",
        "\n",
        "    # Add experiences to memory buffer\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        max_priority = max(self.priorities) if self.priorities else 1.0\n",
        "\n",
        "        experience = (state, action, reward, next_state, done)\n",
        "\n",
        "        # Replace old memory if at capacity\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(experience)\n",
        "            self.priorities.append(max_priority)\n",
        "        else:\n",
        "            self.buffer[self.position] = experience\n",
        "            self.priorities[self.position] = max_priority\n",
        "\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        if len(self.buffer) == 0:\n",
        "            return None\n",
        "\n",
        "        # Meat and potatoes of PER right here\n",
        "        priorities = np.array(self.priorities[:len(self.buffer)])\n",
        "        probs = priorities ** self.alpha\n",
        "        probs /= probs.sum()\n",
        "\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs, replace=False)\n",
        "\n",
        "        # Importance sampling weights\n",
        "        total = len(self.buffer)\n",
        "        weights = (total * probs[indices]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "\n",
        "        experiences = [self.buffer[idx] for idx in indices]\n",
        "\n",
        "        # Convert to tensors for PyTorch digestion\n",
        "        states = torch.FloatTensor([e[0] for e in experiences]).to(device)\n",
        "        actions = torch.LongTensor([e[1] for e in experiences]).to(device)\n",
        "        rewards = torch.FloatTensor([e[2] for e in experiences]).to(device)\n",
        "        next_states = torch.FloatTensor([e[3] for e in experiences]).to(device)\n",
        "        dones = torch.FloatTensor([e[4] for e in experiences]).to(device)\n",
        "        weights_tensor = torch.FloatTensor(weights).to(device)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones, weights_tensor, indices\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            self.priorities[idx] = priority\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZYjV3xBQeds"
      },
      "source": [
        "## DQN Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fUR16bSQeds"
      },
      "outputs": [],
      "source": [
        "class YahtzeeAgent:\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Initialize networks\n",
        "        self.q_network = DuelingDQN(state_size, action_size, hidden_size=512).to(device)\n",
        "        self.target_network = DuelingDQN(state_size, action_size, hidden_size=512).to(device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        # Optimizer with multi-step Learning Rate decay\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.0001)\n",
        "        self.scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "            self.optimizer,\n",
        "            milestones=[10000, 20000, 30000],  # Drop LR at these episodes\n",
        "            gamma=0.5  # Multiply LR by 0.5 at each milestone\n",
        "        )\n",
        "\n",
        "        # Replay buffer\n",
        "        self.memory = PrioritizedReplayBuffer(capacity=300000, alpha=0.6)\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.epsilon = 1.0 # Initial rate of random exploration\n",
        "        self.epsilon_min = 0.01 # Target minimum exploration rate\n",
        "        self.epsilon_decay = 0.9999  # Slow decay for better exploration\n",
        "        self.gamma = 0.99\n",
        "        self.batch_size = 256 # Could probably go higher, but I don't want to toast the GPUs\n",
        "        self.update_target_every = 500\n",
        "        self.train_start = 10000  # Warmup period\n",
        "        self.steps = 0\n",
        "\n",
        "        # Action mapping\n",
        "        self.action_map = self._create_action_map()\n",
        "\n",
        "        # Training metrics\n",
        "        self.training_metrics = {\n",
        "            'q_values': [],\n",
        "            'td_errors': [],\n",
        "            'gradient_norms': []\n",
        "        }\n",
        "\n",
        "    # Figure out every possible action and map it to an index - this was annoying to figure out\n",
        "    def _create_action_map(self):\n",
        "        action_map = []\n",
        "        for i in range(32):\n",
        "            keep_mask = tuple((i >> j) & 1 for j in range(5))\n",
        "            action_map.append(('roll', keep_mask))\n",
        "        for cat in CATEGORIES:\n",
        "            action_map.append(('score', cat))\n",
        "        return action_map\n",
        "\n",
        "    def get_action_index(self, action):\n",
        "        try:\n",
        "            return self.action_map.index(action)\n",
        "        except ValueError:\n",
        "            return 0\n",
        "\n",
        "    def select_action(self, state, valid_actions, training=True):\n",
        "        # Epsilon-greedy action selection\n",
        "        if training and random.random() < self.epsilon:\n",
        "            return random.choice(valid_actions)\n",
        "\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_values = self.q_network(state_tensor).cpu().numpy()[0]\n",
        "\n",
        "        # Mask invalid actions\n",
        "        valid_indices = [self.get_action_index(a) for a in valid_actions]\n",
        "        masked_q = np.full(self.action_size, -1e10)\n",
        "        masked_q[valid_indices] = q_values[valid_indices]\n",
        "\n",
        "        best_action_idx = np.argmax(masked_q)\n",
        "        return self.action_map[best_action_idx]\n",
        "\n",
        "    def train_step(self, beta=0.4):\n",
        "        # Single training step with prioritized experience replay\n",
        "        if len(self.memory) < self.train_start:\n",
        "            return 0.0\n",
        "\n",
        "        batch = self.memory.sample(self.batch_size, beta)\n",
        "        if batch is None:\n",
        "            return 0.0\n",
        "\n",
        "        states, actions, rewards, next_states, dones, weights, indices = batch\n",
        "\n",
        "        # Current Q values\n",
        "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "        # Double DQN target to prevent overestimation\n",
        "        with torch.no_grad():\n",
        "            next_actions = self.q_network(next_states).max(1)[1]\n",
        "            next_q = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
        "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
        "\n",
        "        # TD errors for priority updates\n",
        "        td_errors = torch.abs(current_q - target_q).detach().cpu().numpy()\n",
        "\n",
        "        # Weighted Huber loss (stablizes training)\n",
        "        loss = (weights * nn.SmoothL1Loss(reduction='none')(current_q, target_q)).mean()\n",
        "\n",
        "        # Optimize and implement gradient clipping\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update priorities (with small epsilon to avoid zero priority)\n",
        "        self.memory.update_priorities(indices, td_errors + 1e-6)\n",
        "\n",
        "        # Update target network\n",
        "        self.steps += 1\n",
        "        if self.steps % self.update_target_every == 0:\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        # Decay epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        # Track metrics\n",
        "        self.training_metrics['td_errors'].append(td_errors.mean())\n",
        "        self.training_metrics['gradient_norms'].append(grad_norm.item())\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    # Save and load model functions\n",
        "    def save(self, filepath):\n",
        "        torch.save({\n",
        "            'q_network': self.q_network.state_dict(),\n",
        "            'target_network': self.target_network.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'scheduler': self.scheduler.state_dict(),\n",
        "            'epsilon': self.epsilon,\n",
        "            'steps': self.steps,\n",
        "            'training_metrics': self.training_metrics\n",
        "        }, filepath)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "\n",
        "    def load(self, filepath):\n",
        "        checkpoint = torch.load(filepath, map_location=device)\n",
        "        self.q_network.load_state_dict(checkpoint['q_network'])\n",
        "        self.target_network.load_state_dict(checkpoint['target_network'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        self.scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "        self.epsilon = checkpoint['epsilon']\n",
        "        self.steps = checkpoint['steps']\n",
        "        self.training_metrics = checkpoint.get('training_metrics', self.training_metrics)\n",
        "        print(f\"Model loaded from {filepath}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnOYRlbHQedt"
      },
      "source": [
        "## Reward Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99JOgILAQedt"
      },
      "outputs": [],
      "source": [
        "\"\"\"This part is incredibly important - the reward function defines how the agent learns!\n",
        "I didn't know exactly how to do it at first, so I iterated by running it about 20% of the way several times.\n",
        "However, I eventually just asked ChatGPT to rewrite it to be accurate to the probability of Yahtzee and to\n",
        "reward the agent accordingly. The result is a much better reward function that leads to far superior play.\n",
        "\"\"\"\n",
        "def calculate_reward(game, action_type, score, prev_score, category_used):\n",
        "\n",
        "    # ---- 1. Reward for rolling ----\n",
        "    if action_type == 'roll':\n",
        "        return 0.0  # rolling shouldn't bias the value function\n",
        "\n",
        "    # ---- 2. If the score is invalid ----\n",
        "    if score < 0:\n",
        "        return -10.0  # keep penalty small but discouraging\n",
        "\n",
        "    reward = 0.0\n",
        "\n",
        "    # Category expected values (approx across optimal play)\n",
        "    EV = {\n",
        "        \"ones\":   2.8,\n",
        "        \"twos\":   5.6,\n",
        "        \"threes\": 8.5,\n",
        "        \"fours\":  11.3,\n",
        "        \"fives\":  14.0,\n",
        "        \"sixes\":  16.8,\n",
        "        \"three_of_kind\": 12,\n",
        "        \"four_of_kind\":  7,\n",
        "        \"full_house\": 9.2,\n",
        "        \"small_straight\": 10,\n",
        "        \"large_straight\": 7.5,\n",
        "        \"yahtzee\": 2.5,\n",
        "        \"chance\": 23\n",
        "    }\n",
        "\n",
        "    # Reward based on improvement over the EV for that category\n",
        "    expected = EV.get(category_used, 0)\n",
        "    reward += (score - expected) / 10.0\n",
        "    #       ^ normalization so the agent sees consistent gradients\n",
        "\n",
        "    if category_used in CATEGORIES[:6]:\n",
        "        upper_sum = sum(game.scorecard[cat] if game.scorecard[cat] is not None else 0\n",
        "                       for cat in CATEGORIES[:6])\n",
        "        before = upper_sum - score   # previous total\n",
        "        after  = upper_sum           # new total\n",
        "\n",
        "        # Reward progress normalized by 63-point target\n",
        "        upper_progress = (after - before) / 63.0\n",
        "        reward += upper_progress * 3.0  # strong encouragement\n",
        "\n",
        "    if score == 0:\n",
        "\n",
        "        # Zeroing Yahtzee or 1s is fine\n",
        "        if category_used in (\"yahtzee\", \"ones\"):\n",
        "            reward -= 0.1  # almost neutral\n",
        "\n",
        "        # Zeroing Chance early is bad\n",
        "        elif category_used == \"chance\":\n",
        "            reward -= 2.0\n",
        "\n",
        "        # Zeroing upper categories mid-game is costly\n",
        "        elif category_used in CATEGORIES[:6]:\n",
        "            reward -= 1.5\n",
        "\n",
        "        else:\n",
        "            reward -= 0.5  # reasonable penalty\n",
        "\n",
        "    # Add a small rarity reward (helpful early)\n",
        "    rarity_bonus = {\n",
        "        \"yahtzee\": 1.5,\n",
        "        \"large_straight\": 0.9,\n",
        "        \"small_straight\": 0.4,\n",
        "        \"full_house\": 0.15,\n",
        "    }\n",
        "\n",
        "    if category_used in rarity_bonus and score > 0:\n",
        "        reward += rarity_bonus[category_used]\n",
        "\n",
        "    if game.is_game_over():\n",
        "        total = game.total_score\n",
        "\n",
        "        # Nonlinear shaping—smooth and RL-friendly\n",
        "        if total >= 300:\n",
        "            reward += 20.0\n",
        "        elif total >= 250:\n",
        "            reward += 12.0\n",
        "        elif total >= 200:\n",
        "            reward += 6.0\n",
        "        elif total >= 150:\n",
        "            reward += 2.0\n",
        "        else:\n",
        "            reward -= 5.0\n",
        "\n",
        "\n",
        "    return reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u68YduqrQedu"
      },
      "source": [
        "## Training and Testing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv_c6ThtQedu"
      },
      "outputs": [],
      "source": [
        "def play_episode(game, agent, training=True, verbose=False):\n",
        "    # Play one complete Yahtzee game (this was fun to write)\n",
        "    state = game.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "\n",
        "    while not game.is_game_over() and steps < 250: # steps < 250 not needed, but a safety net for infinite loops\n",
        "        valid_actions = game.get_valid_actions()\n",
        "        if not valid_actions:\n",
        "            break\n",
        "\n",
        "        action = agent.select_action(state, valid_actions, training)\n",
        "        action_type, action_value = action\n",
        "\n",
        "        prev_score = game.total_score\n",
        "\n",
        "        if action_type == 'roll':\n",
        "            game.roll_dice(keep_mask=action_value)\n",
        "            score = 0\n",
        "            done = False\n",
        "            category_used = None\n",
        "        else:\n",
        "            score = game.score_category(action_value)\n",
        "            done = game.is_game_over()\n",
        "            category_used = action_value\n",
        "            if score < 0:\n",
        "                done = True\n",
        "\n",
        "        next_state = game.get_state()\n",
        "        reward = calculate_reward(game, action_type, score, prev_score, category_used)\n",
        "\n",
        "        if training:\n",
        "            action_idx = agent.get_action_index(action)\n",
        "            agent.memory.add(state, action_idx, reward, next_state, done)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return game.total_score, total_reward, steps\n",
        "\n",
        "\n",
        "def train_agent(episodes=50000, save_path='yahtzee_model.pth', checkpoint_interval=1000):\n",
        "\n",
        "    game = YahtzeeGame()\n",
        "    state_size = len(game.get_state())\n",
        "    action_size = 32 + 13\n",
        "\n",
        "    agent = YahtzeeAgent(state_size, action_size)\n",
        "\n",
        "    # Try to load existing checkpoint\n",
        "    checkpoint_path = 'checkpoint.pth'\n",
        "    start_episode = 0\n",
        "    log_rows = []\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        try:\n",
        "            checkpoint = torch.load(checkpoint_path)\n",
        "            # Load agent state\n",
        "            agent.q_network.load_state_dict(checkpoint['q_network'])\n",
        "            agent.target_network.load_state_dict(checkpoint['target_network'])\n",
        "            agent.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            agent.scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "            agent.epsilon = checkpoint['epsilon']\n",
        "            agent.steps = checkpoint['steps']\n",
        "            agent.training_metrics = checkpoint.get('training_metrics', agent.training_metrics)\n",
        "            # Load training progress\n",
        "            start_episode = checkpoint.get('episode', 0)\n",
        "            log_rows = checkpoint.get('log_rows', [])\n",
        "            print(f\"Resumed from episode {start_episode}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load checkpoint: {e}\")\n",
        "            print(\"Starting from scratch...\")\n",
        "\n",
        "    recent_scores = deque(maxlen=100)\n",
        "    losses = []\n",
        "\n",
        "    print(\"\\nStarting training...\\n\")\n",
        "\n",
        "    for episode in range(start_episode, episodes):\n",
        "\n",
        "        score, reward, steps_taken = play_episode(game, agent, training=True)\n",
        "        recent_scores.append(score)\n",
        "\n",
        "        beta = min(1.0, 0.4 + 0.6 * episode / episodes)\n",
        "        loss = agent.train_step(beta)\n",
        "        if loss > 0:\n",
        "            losses.append(loss)\n",
        "\n",
        "        agent.scheduler.step()\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_score = np.mean(recent_scores)\n",
        "            avg_loss = (np.mean(losses[-500:])\n",
        "                        if len(losses) >= 500\n",
        "                        else (np.mean(losses) if losses else 0))\n",
        "            lr = agent.optimizer.param_groups[0]['lr']\n",
        "\n",
        "            print(f\"[Episode {episode+1}] \"\n",
        "                  f\"AvgScore={avg_score:.1f}  \"\n",
        "                  f\"Loss={avg_loss:.4f}  \"\n",
        "                  f\"Eps={agent.epsilon:.4f}  \"\n",
        "                  f\"LR={lr:.6f}  \"\n",
        "                  f\"Steps={agent.steps}\")\n",
        "\n",
        "        lr = agent.optimizer.param_groups[0]['lr']\n",
        "        log_rows.append({\n",
        "            'episode': episode + 1,\n",
        "            'score': score,\n",
        "            'reward': reward,\n",
        "            'steps': steps_taken,\n",
        "            'epsilon': agent.epsilon,\n",
        "            'learning_rate': lr,\n",
        "            'loss': float(loss if loss > 0 else 0.0)\n",
        "        })\n",
        "\n",
        "        # Save checkpoint periodically\n",
        "        if (episode + 1) % checkpoint_interval == 0:\n",
        "            checkpoint = {\n",
        "                'q_network': agent.q_network.state_dict(),\n",
        "                'target_network': agent.target_network.state_dict(),\n",
        "                'optimizer': agent.optimizer.state_dict(),\n",
        "                'scheduler': agent.scheduler.state_dict(),\n",
        "                'epsilon': agent.epsilon,\n",
        "                'steps': agent.steps,\n",
        "                'training_metrics': agent.training_metrics,\n",
        "                'episode': episode + 1,\n",
        "                'log_rows': log_rows\n",
        "            }\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "            # Also save CSV incrementally\n",
        "            csv_path = \"training_log.csv\"\n",
        "            with open(csv_path, 'w', newline='') as f:\n",
        "                writer = csv.DictWriter(f, fieldnames=list(log_rows[0].keys()))\n",
        "                writer.writeheader()\n",
        "                writer.writerows(log_rows)\n",
        "\n",
        "            print(f\"Checkpoint saved at episode {episode+1}\")\n",
        "\n",
        "    # Save final model\n",
        "    agent.save(save_path)\n",
        "\n",
        "    # Write final CSV\n",
        "    csv_path = \"training_log.csv\"\n",
        "    with open(csv_path, 'w', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=list(log_rows[0].keys()))\n",
        "        writer.writeheader()\n",
        "        writer.writerows(log_rows)\n",
        "\n",
        "    print(f\"\\n✓ Training complete. CSV saved to {csv_path}\")\n",
        "    print(f\"✓ Model saved to {save_path}\\n\")\n",
        "\n",
        "    return agent\n",
        "\n",
        "\n",
        "def test_agent(agent, num_games=100, verbose=False):\n",
        "    # Run evaluation episodes with epsilon=0 and return scores.\n",
        "    game = YahtzeeGame()\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0\n",
        "\n",
        "    scores = []\n",
        "    rewards = []\n",
        "\n",
        "    for _ in range(num_games):\n",
        "        score, reward, _ = play_episode(game, agent, training=False, verbose=verbose)\n",
        "        scores.append(score)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    # Summary statistics\n",
        "    avg = np.mean(scores)\n",
        "    std = np.std(scores)\n",
        "    med = np.median(scores)\n",
        "\n",
        "    print(f\"\\n=== TEST RESULTS ({num_games} games) ===\")\n",
        "    print(f\"Average: {avg:.1f}\")\n",
        "    print(f\"Std Dev: {std:.1f}\")\n",
        "    print(f\"Median: {med:.1f}\")\n",
        "    print(f\"Min: {min(scores)}\")\n",
        "    print(f\"Max: {max(scores)}\")\n",
        "    print(f\">200: {sum(s >= 200 for s in scores)}/{num_games}\")\n",
        "    print(f\">250: {sum(s >= 250 for s in scores)}/{num_games}\")\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxf0EVIjQedu"
      },
      "source": [
        "## Train the Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A16g524gQedu",
        "outputId": "bf3819b6-e687-4c88-b2bb-ff74c515cdc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n",
            "\n",
            "[Episode 100] AvgScore=49.6  Loss=0.0000  Eps=1.0000  LR=0.000100  Steps=0\n",
            "[Episode 200] AvgScore=44.5  Loss=0.0000  Eps=1.0000  LR=0.000100  Steps=0\n",
            "[Episode 300] AvgScore=45.2  Loss=0.1481  Eps=0.9919  LR=0.000100  Steps=81\n",
            "[Episode 400] AvgScore=43.1  Loss=0.1418  Eps=0.9821  LR=0.000100  Steps=181\n",
            "[Episode 500] AvgScore=46.9  Loss=0.1334  Eps=0.9723  LR=0.000100  Steps=281\n",
            "[Episode 600] AvgScore=47.6  Loss=0.1255  Eps=0.9626  LR=0.000100  Steps=381\n",
            "[Episode 700] AvgScore=51.0  Loss=0.1175  Eps=0.9530  LR=0.000100  Steps=481\n",
            "[Episode 800] AvgScore=52.4  Loss=0.1034  Eps=0.9436  LR=0.000100  Steps=581\n",
            "[Episode 900] AvgScore=50.4  Loss=0.0887  Eps=0.9342  LR=0.000100  Steps=681\n",
            "[Episode 1000] AvgScore=53.5  Loss=0.0762  Eps=0.9249  LR=0.000100  Steps=781\n",
            "Checkpoint saved at episode 1000\n",
            "[Episode 1100] AvgScore=53.7  Loss=0.0662  Eps=0.9157  LR=0.000100  Steps=881\n",
            "[Episode 1200] AvgScore=57.0  Loss=0.0589  Eps=0.9066  LR=0.000100  Steps=981\n",
            "[Episode 1300] AvgScore=53.2  Loss=0.0570  Eps=0.8975  LR=0.000100  Steps=1081\n",
            "[Episode 1400] AvgScore=57.2  Loss=0.0571  Eps=0.8886  LR=0.000100  Steps=1181\n",
            "[Episode 1500] AvgScore=55.7  Loss=0.0583  Eps=0.8798  LR=0.000100  Steps=1281\n",
            "[Episode 1600] AvgScore=56.0  Loss=0.0596  Eps=0.8710  LR=0.000100  Steps=1381\n",
            "[Episode 1700] AvgScore=61.5  Loss=0.0610  Eps=0.8623  LR=0.000100  Steps=1481\n",
            "[Episode 1800] AvgScore=60.4  Loss=0.0603  Eps=0.8538  LR=0.000100  Steps=1581\n",
            "[Episode 1900] AvgScore=62.0  Loss=0.0592  Eps=0.8453  LR=0.000100  Steps=1681\n",
            "[Episode 2000] AvgScore=63.7  Loss=0.0584  Eps=0.8369  LR=0.000100  Steps=1781\n",
            "Checkpoint saved at episode 2000\n",
            "[Episode 2100] AvgScore=63.5  Loss=0.0586  Eps=0.8285  LR=0.000100  Steps=1881\n",
            "[Episode 2200] AvgScore=62.3  Loss=0.0587  Eps=0.8203  LR=0.000100  Steps=1981\n",
            "[Episode 2300] AvgScore=70.4  Loss=0.0586  Eps=0.8121  LR=0.000100  Steps=2081\n",
            "[Episode 2400] AvgScore=66.4  Loss=0.0583  Eps=0.8040  LR=0.000100  Steps=2181\n",
            "[Episode 2500] AvgScore=66.4  Loss=0.0579  Eps=0.7960  LR=0.000100  Steps=2281\n",
            "[Episode 2600] AvgScore=67.5  Loss=0.0568  Eps=0.7881  LR=0.000100  Steps=2381\n",
            "[Episode 2700] AvgScore=68.2  Loss=0.0562  Eps=0.7803  LR=0.000100  Steps=2481\n",
            "[Episode 2800] AvgScore=71.3  Loss=0.0557  Eps=0.7725  LR=0.000100  Steps=2581\n",
            "[Episode 2900] AvgScore=74.3  Loss=0.0560  Eps=0.7648  LR=0.000100  Steps=2681\n",
            "[Episode 3000] AvgScore=72.6  Loss=0.0558  Eps=0.7572  LR=0.000100  Steps=2781\n",
            "Checkpoint saved at episode 3000\n",
            "[Episode 3100] AvgScore=73.6  Loss=0.0559  Eps=0.7497  LR=0.000100  Steps=2881\n",
            "[Episode 3200] AvgScore=75.5  Loss=0.0564  Eps=0.7422  LR=0.000100  Steps=2981\n",
            "[Episode 3300] AvgScore=73.3  Loss=0.0560  Eps=0.7348  LR=0.000100  Steps=3081\n",
            "[Episode 3400] AvgScore=74.7  Loss=0.0543  Eps=0.7275  LR=0.000100  Steps=3181\n",
            "[Episode 3500] AvgScore=77.1  Loss=0.0538  Eps=0.7203  LR=0.000100  Steps=3281\n",
            "[Episode 3600] AvgScore=79.4  Loss=0.0529  Eps=0.7131  LR=0.000100  Steps=3381\n",
            "[Episode 3700] AvgScore=81.5  Loss=0.0516  Eps=0.7060  LR=0.000100  Steps=3481\n",
            "[Episode 3800] AvgScore=78.6  Loss=0.0512  Eps=0.6990  LR=0.000100  Steps=3581\n",
            "[Episode 3900] AvgScore=84.4  Loss=0.0514  Eps=0.6920  LR=0.000100  Steps=3681\n",
            "[Episode 4000] AvgScore=80.0  Loss=0.0509  Eps=0.6851  LR=0.000100  Steps=3781\n",
            "Checkpoint saved at episode 4000\n",
            "[Episode 4100] AvgScore=89.2  Loss=0.0504  Eps=0.6783  LR=0.000100  Steps=3881\n",
            "[Episode 4200] AvgScore=81.6  Loss=0.0499  Eps=0.6716  LR=0.000100  Steps=3981\n",
            "[Episode 4300] AvgScore=81.2  Loss=0.0495  Eps=0.6649  LR=0.000100  Steps=4081\n",
            "[Episode 4400] AvgScore=79.8  Loss=0.0496  Eps=0.6583  LR=0.000100  Steps=4181\n",
            "[Episode 4500] AvgScore=88.0  Loss=0.0500  Eps=0.6517  LR=0.000100  Steps=4281\n",
            "[Episode 4600] AvgScore=86.1  Loss=0.0503  Eps=0.6452  LR=0.000100  Steps=4381\n",
            "[Episode 4700] AvgScore=88.5  Loss=0.0500  Eps=0.6388  LR=0.000100  Steps=4481\n",
            "[Episode 4800] AvgScore=90.5  Loss=0.0497  Eps=0.6325  LR=0.000100  Steps=4581\n",
            "[Episode 4900] AvgScore=90.0  Loss=0.0493  Eps=0.6262  LR=0.000100  Steps=4681\n",
            "[Episode 5000] AvgScore=90.2  Loss=0.0487  Eps=0.6199  LR=0.000100  Steps=4781\n",
            "Checkpoint saved at episode 5000\n",
            "[Episode 5100] AvgScore=90.3  Loss=0.0476  Eps=0.6138  LR=0.000100  Steps=4881\n",
            "[Episode 5200] AvgScore=92.0  Loss=0.0474  Eps=0.6077  LR=0.000100  Steps=4981\n",
            "[Episode 5300] AvgScore=95.3  Loss=0.0467  Eps=0.6016  LR=0.000100  Steps=5081\n",
            "[Episode 5400] AvgScore=89.4  Loss=0.0458  Eps=0.5956  LR=0.000100  Steps=5181\n",
            "[Episode 5500] AvgScore=98.6  Loss=0.0448  Eps=0.5897  LR=0.000100  Steps=5281\n",
            "[Episode 5600] AvgScore=99.2  Loss=0.0441  Eps=0.5838  LR=0.000100  Steps=5381\n",
            "[Episode 5700] AvgScore=95.2  Loss=0.0431  Eps=0.5780  LR=0.000100  Steps=5481\n",
            "[Episode 5800] AvgScore=97.8  Loss=0.0431  Eps=0.5723  LR=0.000100  Steps=5581\n",
            "[Episode 5900] AvgScore=99.7  Loss=0.0436  Eps=0.5666  LR=0.000100  Steps=5681\n",
            "[Episode 6000] AvgScore=97.7  Loss=0.0443  Eps=0.5609  LR=0.000100  Steps=5781\n",
            "Checkpoint saved at episode 6000\n",
            "[Episode 6100] AvgScore=99.2  Loss=0.0447  Eps=0.5554  LR=0.000100  Steps=5881\n",
            "[Episode 6200] AvgScore=97.7  Loss=0.0449  Eps=0.5498  LR=0.000100  Steps=5981\n",
            "[Episode 6300] AvgScore=102.8  Loss=0.0445  Eps=0.5444  LR=0.000100  Steps=6081\n",
            "[Episode 6400] AvgScore=99.4  Loss=0.0437  Eps=0.5390  LR=0.000100  Steps=6181\n",
            "[Episode 6500] AvgScore=99.0  Loss=0.0431  Eps=0.5336  LR=0.000100  Steps=6281\n",
            "[Episode 6600] AvgScore=105.2  Loss=0.0427  Eps=0.5283  LR=0.000100  Steps=6381\n",
            "[Episode 6700] AvgScore=106.0  Loss=0.0427  Eps=0.5230  LR=0.000100  Steps=6481\n",
            "[Episode 6800] AvgScore=95.5  Loss=0.0429  Eps=0.5178  LR=0.000100  Steps=6581\n",
            "[Episode 6900] AvgScore=108.0  Loss=0.0425  Eps=0.5127  LR=0.000100  Steps=6681\n",
            "[Episode 7000] AvgScore=103.3  Loss=0.0429  Eps=0.5076  LR=0.000100  Steps=6781\n",
            "Checkpoint saved at episode 7000\n",
            "[Episode 7100] AvgScore=111.0  Loss=0.0431  Eps=0.5025  LR=0.000100  Steps=6881\n",
            "[Episode 7200] AvgScore=101.8  Loss=0.0434  Eps=0.4975  LR=0.000100  Steps=6981\n",
            "[Episode 7300] AvgScore=106.8  Loss=0.0431  Eps=0.4926  LR=0.000100  Steps=7081\n",
            "[Episode 7400] AvgScore=104.8  Loss=0.0430  Eps=0.4877  LR=0.000100  Steps=7181\n",
            "[Episode 7500] AvgScore=104.2  Loss=0.0423  Eps=0.4828  LR=0.000100  Steps=7281\n",
            "[Episode 7600] AvgScore=108.4  Loss=0.0420  Eps=0.4780  LR=0.000100  Steps=7381\n",
            "[Episode 7700] AvgScore=109.5  Loss=0.0415  Eps=0.4732  LR=0.000100  Steps=7481\n",
            "[Episode 7800] AvgScore=111.3  Loss=0.0414  Eps=0.4685  LR=0.000100  Steps=7581\n",
            "[Episode 7900] AvgScore=107.2  Loss=0.0416  Eps=0.4639  LR=0.000100  Steps=7681\n",
            "[Episode 8000] AvgScore=109.3  Loss=0.0420  Eps=0.4593  LR=0.000100  Steps=7781\n",
            "Checkpoint saved at episode 8000\n",
            "[Episode 8100] AvgScore=112.3  Loss=0.0420  Eps=0.4547  LR=0.000100  Steps=7881\n",
            "[Episode 8200] AvgScore=109.8  Loss=0.0415  Eps=0.4502  LR=0.000100  Steps=7981\n",
            "[Episode 8300] AvgScore=107.4  Loss=0.0413  Eps=0.4457  LR=0.000100  Steps=8081\n",
            "[Episode 8400] AvgScore=109.1  Loss=0.0414  Eps=0.4413  LR=0.000100  Steps=8181\n",
            "[Episode 8500] AvgScore=107.3  Loss=0.0410  Eps=0.4369  LR=0.000100  Steps=8281\n",
            "[Episode 8600] AvgScore=113.7  Loss=0.0405  Eps=0.4325  LR=0.000100  Steps=8381\n",
            "[Episode 8700] AvgScore=113.4  Loss=0.0407  Eps=0.4282  LR=0.000100  Steps=8481\n",
            "[Episode 8800] AvgScore=117.1  Loss=0.0406  Eps=0.4239  LR=0.000100  Steps=8581\n",
            "[Episode 8900] AvgScore=112.5  Loss=0.0404  Eps=0.4197  LR=0.000100  Steps=8681\n",
            "[Episode 9000] AvgScore=112.7  Loss=0.0398  Eps=0.4156  LR=0.000100  Steps=8781\n",
            "Checkpoint saved at episode 9000\n",
            "[Episode 9100] AvgScore=114.1  Loss=0.0402  Eps=0.4114  LR=0.000100  Steps=8881\n",
            "[Episode 9200] AvgScore=116.6  Loss=0.0405  Eps=0.4073  LR=0.000100  Steps=8981\n",
            "[Episode 9300] AvgScore=114.2  Loss=0.0408  Eps=0.4033  LR=0.000100  Steps=9081\n",
            "[Episode 9400] AvgScore=115.3  Loss=0.0407  Eps=0.3993  LR=0.000100  Steps=9181\n",
            "[Episode 9500] AvgScore=115.8  Loss=0.0413  Eps=0.3953  LR=0.000100  Steps=9281\n",
            "[Episode 9600] AvgScore=118.1  Loss=0.0413  Eps=0.3914  LR=0.000100  Steps=9381\n",
            "[Episode 9700] AvgScore=119.2  Loss=0.0407  Eps=0.3875  LR=0.000100  Steps=9481\n",
            "[Episode 9800] AvgScore=114.6  Loss=0.0405  Eps=0.3836  LR=0.000100  Steps=9581\n",
            "[Episode 9900] AvgScore=115.9  Loss=0.0407  Eps=0.3798  LR=0.000100  Steps=9681\n",
            "[Episode 10000] AvgScore=125.4  Loss=0.0404  Eps=0.3760  LR=0.000050  Steps=9781\n",
            "Checkpoint saved at episode 10000\n",
            "[Episode 10100] AvgScore=118.3  Loss=0.0399  Eps=0.3723  LR=0.000050  Steps=9881\n",
            "[Episode 10200] AvgScore=115.0  Loss=0.0406  Eps=0.3686  LR=0.000050  Steps=9981\n",
            "[Episode 10300] AvgScore=121.9  Loss=0.0401  Eps=0.3649  LR=0.000050  Steps=10081\n",
            "[Episode 10400] AvgScore=123.2  Loss=0.0394  Eps=0.3613  LR=0.000050  Steps=10181\n",
            "[Episode 10500] AvgScore=120.3  Loss=0.0391  Eps=0.3577  LR=0.000050  Steps=10281\n",
            "[Episode 10600] AvgScore=123.5  Loss=0.0390  Eps=0.3541  LR=0.000050  Steps=10381\n",
            "[Episode 10700] AvgScore=125.6  Loss=0.0375  Eps=0.3506  LR=0.000050  Steps=10481\n",
            "[Episode 10800] AvgScore=125.8  Loss=0.0371  Eps=0.3471  LR=0.000050  Steps=10581\n",
            "[Episode 10900] AvgScore=125.2  Loss=0.0368  Eps=0.3436  LR=0.000050  Steps=10681\n",
            "[Episode 11000] AvgScore=127.8  Loss=0.0369  Eps=0.3402  LR=0.000050  Steps=10781\n",
            "Checkpoint saved at episode 11000\n",
            "[Episode 11100] AvgScore=121.1  Loss=0.0366  Eps=0.3368  LR=0.000050  Steps=10881\n",
            "[Episode 11200] AvgScore=128.0  Loss=0.0371  Eps=0.3335  LR=0.000050  Steps=10981\n",
            "[Episode 11300] AvgScore=123.7  Loss=0.0374  Eps=0.3302  LR=0.000050  Steps=11081\n",
            "[Episode 11400] AvgScore=126.3  Loss=0.0374  Eps=0.3269  LR=0.000050  Steps=11181\n",
            "[Episode 11500] AvgScore=126.0  Loss=0.0369  Eps=0.3236  LR=0.000050  Steps=11281\n",
            "[Episode 11600] AvgScore=126.0  Loss=0.0373  Eps=0.3204  LR=0.000050  Steps=11381\n",
            "[Episode 11700] AvgScore=122.4  Loss=0.0370  Eps=0.3172  LR=0.000050  Steps=11481\n",
            "[Episode 11800] AvgScore=130.1  Loss=0.0369  Eps=0.3141  LR=0.000050  Steps=11581\n",
            "[Episode 11900] AvgScore=126.9  Loss=0.0366  Eps=0.3109  LR=0.000050  Steps=11681\n",
            "[Episode 12000] AvgScore=129.6  Loss=0.0364  Eps=0.3078  LR=0.000050  Steps=11781\n",
            "Checkpoint saved at episode 12000\n",
            "[Episode 12100] AvgScore=130.2  Loss=0.0354  Eps=0.3048  LR=0.000050  Steps=11881\n",
            "[Episode 12200] AvgScore=126.0  Loss=0.0352  Eps=0.3017  LR=0.000050  Steps=11981\n",
            "[Episode 12300] AvgScore=126.5  Loss=0.0350  Eps=0.2987  LR=0.000050  Steps=12081\n",
            "[Episode 12400] AvgScore=128.8  Loss=0.0349  Eps=0.2958  LR=0.000050  Steps=12181\n",
            "[Episode 12500] AvgScore=130.1  Loss=0.0347  Eps=0.2928  LR=0.000050  Steps=12281\n",
            "[Episode 12600] AvgScore=126.6  Loss=0.0352  Eps=0.2899  LR=0.000050  Steps=12381\n",
            "[Episode 12700] AvgScore=130.8  Loss=0.0353  Eps=0.2870  LR=0.000050  Steps=12481\n",
            "[Episode 12800] AvgScore=129.1  Loss=0.0350  Eps=0.2842  LR=0.000050  Steps=12581\n",
            "[Episode 12900] AvgScore=132.8  Loss=0.0352  Eps=0.2813  LR=0.000050  Steps=12681\n",
            "[Episode 13000] AvgScore=132.7  Loss=0.0354  Eps=0.2785  LR=0.000050  Steps=12781\n",
            "Checkpoint saved at episode 13000\n",
            "[Episode 13100] AvgScore=128.4  Loss=0.0355  Eps=0.2758  LR=0.000050  Steps=12881\n",
            "[Episode 13200] AvgScore=134.5  Loss=0.0354  Eps=0.2730  LR=0.000050  Steps=12981\n",
            "[Episode 13300] AvgScore=133.1  Loss=0.0349  Eps=0.2703  LR=0.000050  Steps=13081\n",
            "[Episode 13400] AvgScore=132.5  Loss=0.0347  Eps=0.2676  LR=0.000050  Steps=13181\n",
            "[Episode 13500] AvgScore=133.2  Loss=0.0345  Eps=0.2650  LR=0.000050  Steps=13281\n",
            "[Episode 13600] AvgScore=134.1  Loss=0.0343  Eps=0.2623  LR=0.000050  Steps=13381\n",
            "[Episode 13700] AvgScore=141.7  Loss=0.0341  Eps=0.2597  LR=0.000050  Steps=13481\n",
            "[Episode 13800] AvgScore=132.3  Loss=0.0346  Eps=0.2571  LR=0.000050  Steps=13581\n",
            "[Episode 13900] AvgScore=128.7  Loss=0.0346  Eps=0.2546  LR=0.000050  Steps=13681\n",
            "[Episode 14000] AvgScore=132.8  Loss=0.0345  Eps=0.2520  LR=0.000050  Steps=13781\n",
            "Checkpoint saved at episode 14000\n",
            "[Episode 14100] AvgScore=130.9  Loss=0.0341  Eps=0.2495  LR=0.000050  Steps=13881\n",
            "[Episode 14200] AvgScore=134.4  Loss=0.0338  Eps=0.2470  LR=0.000050  Steps=13981\n",
            "[Episode 14300] AvgScore=144.0  Loss=0.0335  Eps=0.2446  LR=0.000050  Steps=14081\n",
            "[Episode 14400] AvgScore=137.9  Loss=0.0329  Eps=0.2422  LR=0.000050  Steps=14181\n",
            "[Episode 14500] AvgScore=142.2  Loss=0.0325  Eps=0.2397  LR=0.000050  Steps=14281\n",
            "[Episode 14600] AvgScore=140.3  Loss=0.0324  Eps=0.2374  LR=0.000050  Steps=14381\n",
            "[Episode 14700] AvgScore=135.3  Loss=0.0323  Eps=0.2350  LR=0.000050  Steps=14481\n",
            "[Episode 14800] AvgScore=136.6  Loss=0.0323  Eps=0.2327  LR=0.000050  Steps=14581\n",
            "[Episode 14900] AvgScore=137.4  Loss=0.0321  Eps=0.2303  LR=0.000050  Steps=14681\n",
            "[Episode 15000] AvgScore=137.2  Loss=0.0319  Eps=0.2281  LR=0.000050  Steps=14781\n",
            "Checkpoint saved at episode 15000\n",
            "[Episode 15100] AvgScore=136.9  Loss=0.0317  Eps=0.2258  LR=0.000050  Steps=14881\n",
            "[Episode 15200] AvgScore=144.7  Loss=0.0316  Eps=0.2235  LR=0.000050  Steps=14981\n",
            "[Episode 15300] AvgScore=138.7  Loss=0.0312  Eps=0.2213  LR=0.000050  Steps=15081\n",
            "[Episode 15400] AvgScore=142.6  Loss=0.0316  Eps=0.2191  LR=0.000050  Steps=15181\n",
            "[Episode 15500] AvgScore=139.5  Loss=0.0319  Eps=0.2169  LR=0.000050  Steps=15281\n",
            "[Episode 15600] AvgScore=141.3  Loss=0.0322  Eps=0.2148  LR=0.000050  Steps=15381\n",
            "[Episode 15700] AvgScore=139.0  Loss=0.0320  Eps=0.2126  LR=0.000050  Steps=15481\n",
            "[Episode 15800] AvgScore=143.7  Loss=0.0320  Eps=0.2105  LR=0.000050  Steps=15581\n",
            "[Episode 15900] AvgScore=139.4  Loss=0.0319  Eps=0.2084  LR=0.000050  Steps=15681\n",
            "[Episode 16000] AvgScore=143.3  Loss=0.0314  Eps=0.2064  LR=0.000050  Steps=15781\n",
            "Checkpoint saved at episode 16000\n",
            "[Episode 16100] AvgScore=134.9  Loss=0.0311  Eps=0.2043  LR=0.000050  Steps=15881\n",
            "[Episode 16200] AvgScore=138.8  Loss=0.0307  Eps=0.2023  LR=0.000050  Steps=15981\n",
            "[Episode 16300] AvgScore=141.1  Loss=0.0308  Eps=0.2003  LR=0.000050  Steps=16081\n",
            "[Episode 16400] AvgScore=140.9  Loss=0.0308  Eps=0.1983  LR=0.000050  Steps=16181\n",
            "[Episode 16500] AvgScore=138.2  Loss=0.0310  Eps=0.1963  LR=0.000050  Steps=16281\n",
            "[Episode 16600] AvgScore=147.1  Loss=0.0309  Eps=0.1943  LR=0.000050  Steps=16381\n",
            "[Episode 16700] AvgScore=144.9  Loss=0.0307  Eps=0.1924  LR=0.000050  Steps=16481\n",
            "[Episode 16800] AvgScore=139.0  Loss=0.0303  Eps=0.1905  LR=0.000050  Steps=16581\n",
            "[Episode 16900] AvgScore=136.7  Loss=0.0297  Eps=0.1886  LR=0.000050  Steps=16681\n",
            "[Episode 17000] AvgScore=139.4  Loss=0.0293  Eps=0.1867  LR=0.000050  Steps=16781\n",
            "Checkpoint saved at episode 17000\n",
            "[Episode 17100] AvgScore=145.0  Loss=0.0287  Eps=0.1849  LR=0.000050  Steps=16881\n",
            "[Episode 17200] AvgScore=140.2  Loss=0.0290  Eps=0.1830  LR=0.000050  Steps=16981\n",
            "[Episode 17300] AvgScore=146.2  Loss=0.0289  Eps=0.1812  LR=0.000050  Steps=17081\n",
            "[Episode 17400] AvgScore=147.9  Loss=0.0288  Eps=0.1794  LR=0.000050  Steps=17181\n",
            "[Episode 17500] AvgScore=144.5  Loss=0.0289  Eps=0.1776  LR=0.000050  Steps=17281\n",
            "[Episode 17600] AvgScore=145.7  Loss=0.0288  Eps=0.1758  LR=0.000050  Steps=17381\n",
            "[Episode 17700] AvgScore=143.9  Loss=0.0286  Eps=0.1741  LR=0.000050  Steps=17481\n",
            "[Episode 17800] AvgScore=143.3  Loss=0.0283  Eps=0.1724  LR=0.000050  Steps=17581\n",
            "[Episode 17900] AvgScore=138.7  Loss=0.0283  Eps=0.1706  LR=0.000050  Steps=17681\n",
            "[Episode 18000] AvgScore=142.3  Loss=0.0279  Eps=0.1689  LR=0.000050  Steps=17781\n",
            "Checkpoint saved at episode 18000\n",
            "[Episode 18100] AvgScore=144.9  Loss=0.0279  Eps=0.1673  LR=0.000050  Steps=17881\n",
            "[Episode 18200] AvgScore=148.3  Loss=0.0276  Eps=0.1656  LR=0.000050  Steps=17981\n",
            "[Episode 18300] AvgScore=146.8  Loss=0.0274  Eps=0.1640  LR=0.000050  Steps=18081\n",
            "[Episode 18400] AvgScore=144.2  Loss=0.0273  Eps=0.1623  LR=0.000050  Steps=18181\n",
            "[Episode 18500] AvgScore=151.0  Loss=0.0275  Eps=0.1607  LR=0.000050  Steps=18281\n",
            "[Episode 18600] AvgScore=145.2  Loss=0.0278  Eps=0.1591  LR=0.000050  Steps=18381\n",
            "[Episode 18700] AvgScore=144.0  Loss=0.0277  Eps=0.1575  LR=0.000050  Steps=18481\n",
            "[Episode 18800] AvgScore=148.3  Loss=0.0278  Eps=0.1560  LR=0.000050  Steps=18581\n",
            "[Episode 18900] AvgScore=146.8  Loss=0.0279  Eps=0.1544  LR=0.000050  Steps=18681\n",
            "[Episode 19000] AvgScore=146.5  Loss=0.0277  Eps=0.1529  LR=0.000050  Steps=18781\n",
            "Checkpoint saved at episode 19000\n",
            "[Episode 19100] AvgScore=148.5  Loss=0.0276  Eps=0.1513  LR=0.000050  Steps=18881\n",
            "[Episode 19200] AvgScore=147.3  Loss=0.0275  Eps=0.1498  LR=0.000050  Steps=18981\n",
            "[Episode 19300] AvgScore=146.9  Loss=0.0274  Eps=0.1483  LR=0.000050  Steps=19081\n",
            "[Episode 19400] AvgScore=146.0  Loss=0.0275  Eps=0.1469  LR=0.000050  Steps=19181\n",
            "[Episode 19500] AvgScore=141.3  Loss=0.0275  Eps=0.1454  LR=0.000050  Steps=19281\n",
            "[Episode 19600] AvgScore=153.0  Loss=0.0271  Eps=0.1440  LR=0.000050  Steps=19381\n",
            "[Episode 19700] AvgScore=145.3  Loss=0.0271  Eps=0.1425  LR=0.000050  Steps=19481\n",
            "[Episode 19800] AvgScore=150.9  Loss=0.0267  Eps=0.1411  LR=0.000050  Steps=19581\n",
            "[Episode 19900] AvgScore=150.8  Loss=0.0267  Eps=0.1397  LR=0.000050  Steps=19681\n",
            "[Episode 20000] AvgScore=146.4  Loss=0.0261  Eps=0.1383  LR=0.000025  Steps=19781\n",
            "Checkpoint saved at episode 20000\n",
            "[Episode 20100] AvgScore=147.1  Loss=0.0261  Eps=0.1369  LR=0.000025  Steps=19881\n",
            "[Episode 20200] AvgScore=154.7  Loss=0.0258  Eps=0.1356  LR=0.000025  Steps=19981\n",
            "[Episode 20300] AvgScore=152.6  Loss=0.0259  Eps=0.1342  LR=0.000025  Steps=20081\n",
            "[Episode 20400] AvgScore=154.2  Loss=0.0255  Eps=0.1329  LR=0.000025  Steps=20181\n",
            "[Episode 20500] AvgScore=148.9  Loss=0.0255  Eps=0.1316  LR=0.000025  Steps=20281\n",
            "[Episode 20600] AvgScore=155.8  Loss=0.0251  Eps=0.1303  LR=0.000025  Steps=20381\n",
            "[Episode 20700] AvgScore=150.6  Loss=0.0249  Eps=0.1290  LR=0.000025  Steps=20481\n",
            "[Episode 20800] AvgScore=153.3  Loss=0.0248  Eps=0.1277  LR=0.000025  Steps=20581\n",
            "[Episode 20900] AvgScore=153.0  Loss=0.0249  Eps=0.1264  LR=0.000025  Steps=20681\n",
            "[Episode 21000] AvgScore=154.6  Loss=0.0247  Eps=0.1252  LR=0.000025  Steps=20781\n",
            "Checkpoint saved at episode 21000\n",
            "[Episode 21100] AvgScore=151.2  Loss=0.0250  Eps=0.1239  LR=0.000025  Steps=20881\n",
            "[Episode 21200] AvgScore=156.4  Loss=0.0249  Eps=0.1227  LR=0.000025  Steps=20981\n",
            "[Episode 21300] AvgScore=156.9  Loss=0.0247  Eps=0.1215  LR=0.000025  Steps=21081\n",
            "[Episode 21400] AvgScore=152.3  Loss=0.0245  Eps=0.1202  LR=0.000025  Steps=21181\n",
            "[Episode 21500] AvgScore=151.9  Loss=0.0243  Eps=0.1191  LR=0.000025  Steps=21281\n",
            "[Episode 21600] AvgScore=149.5  Loss=0.0238  Eps=0.1179  LR=0.000025  Steps=21381\n",
            "[Episode 21700] AvgScore=153.7  Loss=0.0238  Eps=0.1167  LR=0.000025  Steps=21481\n",
            "[Episode 21800] AvgScore=151.6  Loss=0.0238  Eps=0.1155  LR=0.000025  Steps=21581\n",
            "[Episode 21900] AvgScore=157.7  Loss=0.0234  Eps=0.1144  LR=0.000025  Steps=21681\n",
            "[Episode 22000] AvgScore=153.2  Loss=0.0238  Eps=0.1132  LR=0.000025  Steps=21781\n",
            "Checkpoint saved at episode 22000\n",
            "[Episode 22100] AvgScore=150.9  Loss=0.0241  Eps=0.1121  LR=0.000025  Steps=21881\n",
            "[Episode 22200] AvgScore=160.0  Loss=0.0239  Eps=0.1110  LR=0.000025  Steps=21981\n",
            "[Episode 22300] AvgScore=149.8  Loss=0.0237  Eps=0.1099  LR=0.000025  Steps=22081\n",
            "[Episode 22400] AvgScore=150.4  Loss=0.0238  Eps=0.1088  LR=0.000025  Steps=22181\n",
            "[Episode 22500] AvgScore=158.6  Loss=0.0237  Eps=0.1077  LR=0.000025  Steps=22281\n",
            "[Episode 22600] AvgScore=154.0  Loss=0.0235  Eps=0.1066  LR=0.000025  Steps=22381\n",
            "[Episode 22700] AvgScore=155.4  Loss=0.0237  Eps=0.1056  LR=0.000025  Steps=22481\n",
            "[Episode 22800] AvgScore=155.1  Loss=0.0239  Eps=0.1045  LR=0.000025  Steps=22581\n",
            "[Episode 22900] AvgScore=153.8  Loss=0.0240  Eps=0.1035  LR=0.000025  Steps=22681\n",
            "[Episode 23000] AvgScore=152.4  Loss=0.0236  Eps=0.1025  LR=0.000025  Steps=22781\n",
            "Checkpoint saved at episode 23000\n",
            "[Episode 23100] AvgScore=157.5  Loss=0.0238  Eps=0.1014  LR=0.000025  Steps=22881\n",
            "[Episode 23200] AvgScore=152.1  Loss=0.0237  Eps=0.1004  LR=0.000025  Steps=22981\n",
            "[Episode 23300] AvgScore=160.4  Loss=0.0232  Eps=0.0994  LR=0.000025  Steps=23081\n",
            "[Episode 23400] AvgScore=161.2  Loss=0.0228  Eps=0.0984  LR=0.000025  Steps=23181\n",
            "[Episode 23500] AvgScore=157.3  Loss=0.0227  Eps=0.0975  LR=0.000025  Steps=23281\n",
            "[Episode 23600] AvgScore=154.6  Loss=0.0223  Eps=0.0965  LR=0.000025  Steps=23381\n",
            "[Episode 23700] AvgScore=156.7  Loss=0.0219  Eps=0.0955  LR=0.000025  Steps=23481\n",
            "[Episode 23800] AvgScore=160.5  Loss=0.0222  Eps=0.0946  LR=0.000025  Steps=23581\n",
            "[Episode 23900] AvgScore=160.9  Loss=0.0222  Eps=0.0936  LR=0.000025  Steps=23681\n",
            "[Episode 24000] AvgScore=157.4  Loss=0.0222  Eps=0.0927  LR=0.000025  Steps=23781\n",
            "Checkpoint saved at episode 24000\n",
            "[Episode 24100] AvgScore=160.5  Loss=0.0224  Eps=0.0918  LR=0.000025  Steps=23881\n",
            "[Episode 24200] AvgScore=148.1  Loss=0.0223  Eps=0.0909  LR=0.000025  Steps=23981\n",
            "[Episode 24300] AvgScore=158.1  Loss=0.0226  Eps=0.0900  LR=0.000025  Steps=24081\n",
            "[Episode 24400] AvgScore=154.1  Loss=0.0225  Eps=0.0891  LR=0.000025  Steps=24181\n",
            "[Episode 24500] AvgScore=156.8  Loss=0.0224  Eps=0.0882  LR=0.000025  Steps=24281\n",
            "[Episode 24600] AvgScore=158.3  Loss=0.0219  Eps=0.0873  LR=0.000025  Steps=24381\n",
            "[Episode 24700] AvgScore=155.6  Loss=0.0217  Eps=0.0864  LR=0.000025  Steps=24481\n",
            "[Episode 24800] AvgScore=161.0  Loss=0.0213  Eps=0.0856  LR=0.000025  Steps=24581\n",
            "[Episode 24900] AvgScore=160.1  Loss=0.0212  Eps=0.0847  LR=0.000025  Steps=24681\n",
            "[Episode 25000] AvgScore=155.8  Loss=0.0212  Eps=0.0839  LR=0.000025  Steps=24781\n",
            "Checkpoint saved at episode 25000\n",
            "[Episode 25100] AvgScore=158.6  Loss=0.0216  Eps=0.0831  LR=0.000025  Steps=24881\n",
            "[Episode 25200] AvgScore=161.4  Loss=0.0220  Eps=0.0822  LR=0.000025  Steps=24981\n",
            "[Episode 25300] AvgScore=158.3  Loss=0.0221  Eps=0.0814  LR=0.000025  Steps=25081\n",
            "[Episode 25400] AvgScore=156.4  Loss=0.0219  Eps=0.0806  LR=0.000025  Steps=25181\n",
            "[Episode 25500] AvgScore=158.2  Loss=0.0214  Eps=0.0798  LR=0.000025  Steps=25281\n",
            "[Episode 25600] AvgScore=154.4  Loss=0.0210  Eps=0.0790  LR=0.000025  Steps=25381\n",
            "[Episode 25700] AvgScore=156.5  Loss=0.0209  Eps=0.0782  LR=0.000025  Steps=25481\n",
            "[Episode 25800] AvgScore=160.6  Loss=0.0207  Eps=0.0774  LR=0.000025  Steps=25581\n",
            "[Episode 25900] AvgScore=156.0  Loss=0.0209  Eps=0.0767  LR=0.000025  Steps=25681\n",
            "[Episode 26000] AvgScore=156.4  Loss=0.0212  Eps=0.0759  LR=0.000025  Steps=25781\n",
            "Checkpoint saved at episode 26000\n",
            "[Episode 26100] AvgScore=161.6  Loss=0.0214  Eps=0.0752  LR=0.000025  Steps=25881\n",
            "[Episode 26200] AvgScore=158.1  Loss=0.0213  Eps=0.0744  LR=0.000025  Steps=25981\n",
            "[Episode 26300] AvgScore=160.3  Loss=0.0212  Eps=0.0737  LR=0.000025  Steps=26081\n",
            "[Episode 26400] AvgScore=157.4  Loss=0.0212  Eps=0.0729  LR=0.000025  Steps=26181\n",
            "[Episode 26500] AvgScore=156.1  Loss=0.0211  Eps=0.0722  LR=0.000025  Steps=26281\n",
            "[Episode 26600] AvgScore=160.7  Loss=0.0208  Eps=0.0715  LR=0.000025  Steps=26381\n",
            "[Episode 26700] AvgScore=162.8  Loss=0.0208  Eps=0.0708  LR=0.000025  Steps=26481\n",
            "[Episode 26800] AvgScore=160.2  Loss=0.0206  Eps=0.0701  LR=0.000025  Steps=26581\n",
            "[Episode 26900] AvgScore=157.4  Loss=0.0205  Eps=0.0694  LR=0.000025  Steps=26681\n",
            "[Episode 27000] AvgScore=156.4  Loss=0.0206  Eps=0.0687  LR=0.000025  Steps=26781\n",
            "Checkpoint saved at episode 27000\n",
            "[Episode 27100] AvgScore=161.6  Loss=0.0210  Eps=0.0680  LR=0.000025  Steps=26881\n",
            "[Episode 27200] AvgScore=154.9  Loss=0.0209  Eps=0.0673  LR=0.000025  Steps=26981\n",
            "[Episode 27300] AvgScore=159.3  Loss=0.0206  Eps=0.0667  LR=0.000025  Steps=27081\n",
            "[Episode 27400] AvgScore=158.7  Loss=0.0206  Eps=0.0660  LR=0.000025  Steps=27181\n",
            "[Episode 27500] AvgScore=158.1  Loss=0.0204  Eps=0.0653  LR=0.000025  Steps=27281\n",
            "[Episode 27600] AvgScore=165.1  Loss=0.0200  Eps=0.0647  LR=0.000025  Steps=27381\n",
            "[Episode 27700] AvgScore=157.9  Loss=0.0197  Eps=0.0640  LR=0.000025  Steps=27481\n",
            "[Episode 27800] AvgScore=161.6  Loss=0.0198  Eps=0.0634  LR=0.000025  Steps=27581\n",
            "[Episode 27900] AvgScore=160.8  Loss=0.0193  Eps=0.0628  LR=0.000025  Steps=27681\n",
            "[Episode 28000] AvgScore=154.3  Loss=0.0192  Eps=0.0621  LR=0.000025  Steps=27781\n",
            "Checkpoint saved at episode 28000\n",
            "[Episode 28100] AvgScore=158.5  Loss=0.0192  Eps=0.0615  LR=0.000025  Steps=27881\n",
            "[Episode 28200] AvgScore=158.1  Loss=0.0190  Eps=0.0609  LR=0.000025  Steps=27981\n",
            "[Episode 28300] AvgScore=162.2  Loss=0.0190  Eps=0.0603  LR=0.000025  Steps=28081\n",
            "[Episode 28400] AvgScore=160.8  Loss=0.0194  Eps=0.0597  LR=0.000025  Steps=28181\n",
            "[Episode 28500] AvgScore=160.6  Loss=0.0195  Eps=0.0591  LR=0.000025  Steps=28281\n",
            "[Episode 28600] AvgScore=157.7  Loss=0.0194  Eps=0.0585  LR=0.000025  Steps=28381\n",
            "[Episode 28700] AvgScore=165.9  Loss=0.0193  Eps=0.0579  LR=0.000025  Steps=28481\n",
            "[Episode 28800] AvgScore=163.4  Loss=0.0193  Eps=0.0574  LR=0.000025  Steps=28581\n",
            "[Episode 28900] AvgScore=164.5  Loss=0.0192  Eps=0.0568  LR=0.000025  Steps=28681\n",
            "[Episode 29000] AvgScore=158.6  Loss=0.0192  Eps=0.0562  LR=0.000025  Steps=28781\n",
            "Checkpoint saved at episode 29000\n",
            "[Episode 29100] AvgScore=156.9  Loss=0.0194  Eps=0.0557  LR=0.000025  Steps=28881\n",
            "[Episode 29200] AvgScore=163.6  Loss=0.0196  Eps=0.0551  LR=0.000025  Steps=28981\n",
            "[Episode 29300] AvgScore=163.9  Loss=0.0193  Eps=0.0546  LR=0.000025  Steps=29081\n",
            "[Episode 29400] AvgScore=162.8  Loss=0.0189  Eps=0.0540  LR=0.000025  Steps=29181\n",
            "[Episode 29500] AvgScore=165.7  Loss=0.0188  Eps=0.0535  LR=0.000025  Steps=29281\n",
            "[Episode 29600] AvgScore=161.5  Loss=0.0183  Eps=0.0530  LR=0.000025  Steps=29381\n",
            "[Episode 29700] AvgScore=160.5  Loss=0.0183  Eps=0.0524  LR=0.000025  Steps=29481\n",
            "[Episode 29800] AvgScore=163.4  Loss=0.0184  Eps=0.0519  LR=0.000025  Steps=29581\n",
            "[Episode 29900] AvgScore=164.1  Loss=0.0185  Eps=0.0514  LR=0.000025  Steps=29681\n",
            "[Episode 30000] AvgScore=164.6  Loss=0.0183  Eps=0.0509  LR=0.000013  Steps=29781\n",
            "Checkpoint saved at episode 30000\n",
            "[Episode 30100] AvgScore=165.0  Loss=0.0183  Eps=0.0504  LR=0.000013  Steps=29881\n",
            "[Episode 30200] AvgScore=155.8  Loss=0.0181  Eps=0.0499  LR=0.000013  Steps=29981\n",
            "[Episode 30300] AvgScore=160.8  Loss=0.0180  Eps=0.0494  LR=0.000013  Steps=30081\n",
            "[Episode 30400] AvgScore=165.1  Loss=0.0178  Eps=0.0489  LR=0.000013  Steps=30181\n",
            "[Episode 30500] AvgScore=169.8  Loss=0.0178  Eps=0.0484  LR=0.000013  Steps=30281\n",
            "[Episode 30600] AvgScore=160.8  Loss=0.0177  Eps=0.0479  LR=0.000013  Steps=30381\n",
            "[Episode 30700] AvgScore=164.5  Loss=0.0176  Eps=0.0474  LR=0.000013  Steps=30481\n",
            "[Episode 30800] AvgScore=163.5  Loss=0.0177  Eps=0.0470  LR=0.000013  Steps=30581\n",
            "[Episode 30900] AvgScore=160.0  Loss=0.0178  Eps=0.0465  LR=0.000013  Steps=30681\n",
            "[Episode 31000] AvgScore=168.9  Loss=0.0178  Eps=0.0460  LR=0.000013  Steps=30781\n",
            "Checkpoint saved at episode 31000\n",
            "[Episode 31100] AvgScore=167.0  Loss=0.0181  Eps=0.0456  LR=0.000013  Steps=30881\n",
            "[Episode 31200] AvgScore=168.2  Loss=0.0179  Eps=0.0451  LR=0.000013  Steps=30981\n",
            "[Episode 31300] AvgScore=162.8  Loss=0.0176  Eps=0.0447  LR=0.000013  Steps=31081\n",
            "[Episode 31400] AvgScore=162.3  Loss=0.0175  Eps=0.0442  LR=0.000013  Steps=31181\n",
            "[Episode 31500] AvgScore=167.3  Loss=0.0173  Eps=0.0438  LR=0.000013  Steps=31281\n",
            "[Episode 31600] AvgScore=169.8  Loss=0.0171  Eps=0.0434  LR=0.000013  Steps=31381\n",
            "[Episode 31700] AvgScore=165.4  Loss=0.0171  Eps=0.0429  LR=0.000013  Steps=31481\n",
            "[Episode 31800] AvgScore=170.6  Loss=0.0168  Eps=0.0425  LR=0.000013  Steps=31581\n",
            "[Episode 31900] AvgScore=163.3  Loss=0.0167  Eps=0.0421  LR=0.000013  Steps=31681\n",
            "[Episode 32000] AvgScore=168.4  Loss=0.0167  Eps=0.0417  LR=0.000013  Steps=31781\n",
            "Checkpoint saved at episode 32000\n",
            "[Episode 32100] AvgScore=166.0  Loss=0.0167  Eps=0.0412  LR=0.000013  Steps=31881\n",
            "[Episode 32200] AvgScore=164.1  Loss=0.0165  Eps=0.0408  LR=0.000013  Steps=31981\n",
            "[Episode 32300] AvgScore=161.2  Loss=0.0166  Eps=0.0404  LR=0.000013  Steps=32081\n",
            "[Episode 32400] AvgScore=164.9  Loss=0.0168  Eps=0.0400  LR=0.000013  Steps=32181\n",
            "[Episode 32500] AvgScore=166.8  Loss=0.0169  Eps=0.0396  LR=0.000013  Steps=32281\n",
            "[Episode 32600] AvgScore=164.0  Loss=0.0167  Eps=0.0392  LR=0.000013  Steps=32381\n",
            "[Episode 32700] AvgScore=168.9  Loss=0.0168  Eps=0.0388  LR=0.000013  Steps=32481\n",
            "[Episode 32800] AvgScore=163.6  Loss=0.0170  Eps=0.0385  LR=0.000013  Steps=32581\n",
            "[Episode 32900] AvgScore=163.0  Loss=0.0167  Eps=0.0381  LR=0.000013  Steps=32681\n",
            "[Episode 33000] AvgScore=163.3  Loss=0.0166  Eps=0.0377  LR=0.000013  Steps=32781\n",
            "Checkpoint saved at episode 33000\n",
            "[Episode 33100] AvgScore=166.0  Loss=0.0164  Eps=0.0373  LR=0.000013  Steps=32881\n",
            "[Episode 33200] AvgScore=160.6  Loss=0.0164  Eps=0.0369  LR=0.000013  Steps=32981\n",
            "[Episode 33300] AvgScore=165.2  Loss=0.0162  Eps=0.0366  LR=0.000013  Steps=33081\n",
            "[Episode 33400] AvgScore=164.8  Loss=0.0162  Eps=0.0362  LR=0.000013  Steps=33181\n",
            "[Episode 33500] AvgScore=165.6  Loss=0.0162  Eps=0.0359  LR=0.000013  Steps=33281\n",
            "[Episode 33600] AvgScore=160.4  Loss=0.0164  Eps=0.0355  LR=0.000013  Steps=33381\n",
            "[Episode 33700] AvgScore=159.2  Loss=0.0164  Eps=0.0351  LR=0.000013  Steps=33481\n",
            "[Episode 33800] AvgScore=168.9  Loss=0.0161  Eps=0.0348  LR=0.000013  Steps=33581\n",
            "[Episode 33900] AvgScore=164.2  Loss=0.0162  Eps=0.0344  LR=0.000013  Steps=33681\n",
            "[Episode 34000] AvgScore=167.7  Loss=0.0160  Eps=0.0341  LR=0.000013  Steps=33781\n",
            "Checkpoint saved at episode 34000\n",
            "[Episode 34100] AvgScore=163.0  Loss=0.0160  Eps=0.0338  LR=0.000013  Steps=33881\n",
            "[Episode 34200] AvgScore=164.8  Loss=0.0159  Eps=0.0334  LR=0.000013  Steps=33981\n",
            "[Episode 34300] AvgScore=168.7  Loss=0.0161  Eps=0.0331  LR=0.000013  Steps=34081\n",
            "[Episode 34400] AvgScore=167.1  Loss=0.0161  Eps=0.0328  LR=0.000013  Steps=34181\n",
            "[Episode 34500] AvgScore=163.8  Loss=0.0159  Eps=0.0324  LR=0.000013  Steps=34281\n",
            "[Episode 34600] AvgScore=166.2  Loss=0.0158  Eps=0.0321  LR=0.000013  Steps=34381\n",
            "[Episode 34700] AvgScore=167.2  Loss=0.0156  Eps=0.0318  LR=0.000013  Steps=34481\n",
            "[Episode 34800] AvgScore=166.7  Loss=0.0156  Eps=0.0315  LR=0.000013  Steps=34581\n",
            "[Episode 34900] AvgScore=165.7  Loss=0.0155  Eps=0.0312  LR=0.000013  Steps=34681\n",
            "[Episode 35000] AvgScore=162.3  Loss=0.0155  Eps=0.0309  LR=0.000013  Steps=34781\n",
            "Checkpoint saved at episode 35000\n",
            "[Episode 35100] AvgScore=165.6  Loss=0.0155  Eps=0.0306  LR=0.000013  Steps=34881\n",
            "[Episode 35200] AvgScore=162.6  Loss=0.0157  Eps=0.0302  LR=0.000013  Steps=34981\n",
            "[Episode 35300] AvgScore=165.4  Loss=0.0155  Eps=0.0299  LR=0.000013  Steps=35081\n",
            "[Episode 35400] AvgScore=168.1  Loss=0.0156  Eps=0.0297  LR=0.000013  Steps=35181\n",
            "[Episode 35500] AvgScore=166.0  Loss=0.0157  Eps=0.0294  LR=0.000013  Steps=35281\n",
            "[Episode 35600] AvgScore=169.6  Loss=0.0154  Eps=0.0291  LR=0.000013  Steps=35381\n",
            "[Episode 35700] AvgScore=165.8  Loss=0.0155  Eps=0.0288  LR=0.000013  Steps=35481\n",
            "[Episode 35800] AvgScore=162.3  Loss=0.0154  Eps=0.0285  LR=0.000013  Steps=35581\n",
            "[Episode 35900] AvgScore=164.0  Loss=0.0153  Eps=0.0282  LR=0.000013  Steps=35681\n",
            "[Episode 36000] AvgScore=165.1  Loss=0.0155  Eps=0.0279  LR=0.000013  Steps=35781\n",
            "Checkpoint saved at episode 36000\n",
            "[Episode 36100] AvgScore=166.1  Loss=0.0155  Eps=0.0276  LR=0.000013  Steps=35881\n",
            "[Episode 36200] AvgScore=164.8  Loss=0.0151  Eps=0.0274  LR=0.000013  Steps=35981\n",
            "[Episode 36300] AvgScore=166.7  Loss=0.0153  Eps=0.0271  LR=0.000013  Steps=36081\n",
            "[Episode 36400] AvgScore=165.8  Loss=0.0151  Eps=0.0268  LR=0.000013  Steps=36181\n",
            "[Episode 36500] AvgScore=165.7  Loss=0.0148  Eps=0.0266  LR=0.000013  Steps=36281\n",
            "[Episode 36600] AvgScore=167.2  Loss=0.0150  Eps=0.0263  LR=0.000013  Steps=36381\n",
            "[Episode 36700] AvgScore=167.2  Loss=0.0152  Eps=0.0260  LR=0.000013  Steps=36481\n",
            "[Episode 36800] AvgScore=164.6  Loss=0.0151  Eps=0.0258  LR=0.000013  Steps=36581\n",
            "[Episode 36900] AvgScore=169.6  Loss=0.0150  Eps=0.0255  LR=0.000013  Steps=36681\n",
            "[Episode 37000] AvgScore=163.9  Loss=0.0150  Eps=0.0253  LR=0.000013  Steps=36781\n",
            "Checkpoint saved at episode 37000\n",
            "[Episode 37100] AvgScore=163.9  Loss=0.0148  Eps=0.0250  LR=0.000013  Steps=36881\n",
            "[Episode 37200] AvgScore=166.2  Loss=0.0145  Eps=0.0248  LR=0.000013  Steps=36981\n",
            "[Episode 37300] AvgScore=168.2  Loss=0.0145  Eps=0.0245  LR=0.000013  Steps=37081\n",
            "[Episode 37400] AvgScore=164.9  Loss=0.0142  Eps=0.0243  LR=0.000013  Steps=37181\n",
            "[Episode 37500] AvgScore=161.5  Loss=0.0143  Eps=0.0240  LR=0.000013  Steps=37281\n",
            "[Episode 37600] AvgScore=169.1  Loss=0.0144  Eps=0.0238  LR=0.000013  Steps=37381\n",
            "[Episode 37700] AvgScore=168.3  Loss=0.0145  Eps=0.0236  LR=0.000013  Steps=37481\n",
            "[Episode 37800] AvgScore=160.7  Loss=0.0145  Eps=0.0233  LR=0.000013  Steps=37581\n",
            "[Episode 37900] AvgScore=169.3  Loss=0.0146  Eps=0.0231  LR=0.000013  Steps=37681\n",
            "[Episode 38000] AvgScore=168.7  Loss=0.0145  Eps=0.0229  LR=0.000013  Steps=37781\n",
            "Checkpoint saved at episode 38000\n",
            "[Episode 38100] AvgScore=163.4  Loss=0.0143  Eps=0.0226  LR=0.000013  Steps=37881\n",
            "[Episode 38200] AvgScore=164.3  Loss=0.0142  Eps=0.0224  LR=0.000013  Steps=37981\n",
            "[Episode 38300] AvgScore=167.5  Loss=0.0141  Eps=0.0222  LR=0.000013  Steps=38081\n",
            "[Episode 38400] AvgScore=165.4  Loss=0.0142  Eps=0.0220  LR=0.000013  Steps=38181\n",
            "[Episode 38500] AvgScore=162.9  Loss=0.0144  Eps=0.0217  LR=0.000013  Steps=38281\n",
            "[Episode 38600] AvgScore=167.1  Loss=0.0145  Eps=0.0215  LR=0.000013  Steps=38381\n",
            "[Episode 38700] AvgScore=168.2  Loss=0.0144  Eps=0.0213  LR=0.000013  Steps=38481\n",
            "[Episode 38800] AvgScore=170.9  Loss=0.0144  Eps=0.0211  LR=0.000013  Steps=38581\n",
            "[Episode 38900] AvgScore=166.3  Loss=0.0140  Eps=0.0209  LR=0.000013  Steps=38681\n",
            "[Episode 39000] AvgScore=167.0  Loss=0.0136  Eps=0.0207  LR=0.000013  Steps=38781\n",
            "Checkpoint saved at episode 39000\n",
            "[Episode 39100] AvgScore=169.6  Loss=0.0136  Eps=0.0205  LR=0.000013  Steps=38881\n",
            "[Episode 39200] AvgScore=163.5  Loss=0.0135  Eps=0.0203  LR=0.000013  Steps=38981\n",
            "[Episode 39300] AvgScore=167.6  Loss=0.0134  Eps=0.0201  LR=0.000013  Steps=39081\n",
            "[Episode 39400] AvgScore=168.2  Loss=0.0133  Eps=0.0199  LR=0.000013  Steps=39181\n",
            "[Episode 39500] AvgScore=164.6  Loss=0.0133  Eps=0.0197  LR=0.000013  Steps=39281\n",
            "[Episode 39600] AvgScore=169.6  Loss=0.0129  Eps=0.0195  LR=0.000013  Steps=39381\n",
            "[Episode 39700] AvgScore=166.5  Loss=0.0130  Eps=0.0193  LR=0.000013  Steps=39481\n",
            "[Episode 39800] AvgScore=169.9  Loss=0.0132  Eps=0.0191  LR=0.000013  Steps=39581\n",
            "[Episode 39900] AvgScore=168.6  Loss=0.0135  Eps=0.0189  LR=0.000013  Steps=39681\n",
            "[Episode 40000] AvgScore=166.7  Loss=0.0135  Eps=0.0187  LR=0.000013  Steps=39781\n",
            "Checkpoint saved at episode 40000\n",
            "[Episode 40100] AvgScore=165.5  Loss=0.0138  Eps=0.0185  LR=0.000013  Steps=39881\n",
            "[Episode 40200] AvgScore=171.5  Loss=0.0136  Eps=0.0183  LR=0.000013  Steps=39981\n",
            "[Episode 40300] AvgScore=173.4  Loss=0.0134  Eps=0.0182  LR=0.000013  Steps=40081\n",
            "[Episode 40400] AvgScore=165.6  Loss=0.0132  Eps=0.0180  LR=0.000013  Steps=40181\n",
            "[Episode 40500] AvgScore=166.7  Loss=0.0130  Eps=0.0178  LR=0.000013  Steps=40281\n",
            "[Episode 40600] AvgScore=164.4  Loss=0.0129  Eps=0.0176  LR=0.000013  Steps=40381\n",
            "[Episode 40700] AvgScore=166.6  Loss=0.0131  Eps=0.0175  LR=0.000013  Steps=40481\n",
            "[Episode 40800] AvgScore=165.0  Loss=0.0131  Eps=0.0173  LR=0.000013  Steps=40581\n",
            "[Episode 40900] AvgScore=163.8  Loss=0.0131  Eps=0.0171  LR=0.000013  Steps=40681\n",
            "[Episode 41000] AvgScore=167.3  Loss=0.0131  Eps=0.0169  LR=0.000013  Steps=40781\n",
            "Checkpoint saved at episode 41000\n",
            "[Episode 41100] AvgScore=168.9  Loss=0.0130  Eps=0.0168  LR=0.000013  Steps=40881\n",
            "[Episode 41200] AvgScore=170.5  Loss=0.0127  Eps=0.0166  LR=0.000013  Steps=40981\n",
            "[Episode 41300] AvgScore=168.8  Loss=0.0127  Eps=0.0164  LR=0.000013  Steps=41081\n",
            "[Episode 41400] AvgScore=172.8  Loss=0.0126  Eps=0.0163  LR=0.000013  Steps=41181\n",
            "[Episode 41500] AvgScore=171.4  Loss=0.0125  Eps=0.0161  LR=0.000013  Steps=41281\n",
            "[Episode 41600] AvgScore=173.6  Loss=0.0124  Eps=0.0159  LR=0.000013  Steps=41381\n",
            "[Episode 41700] AvgScore=165.6  Loss=0.0123  Eps=0.0158  LR=0.000013  Steps=41481\n",
            "[Episode 41800] AvgScore=163.2  Loss=0.0123  Eps=0.0156  LR=0.000013  Steps=41581\n",
            "[Episode 41900] AvgScore=163.5  Loss=0.0122  Eps=0.0155  LR=0.000013  Steps=41681\n",
            "[Episode 42000] AvgScore=168.3  Loss=0.0121  Eps=0.0153  LR=0.000013  Steps=41781\n",
            "Checkpoint saved at episode 42000\n",
            "[Episode 42100] AvgScore=163.3  Loss=0.0119  Eps=0.0152  LR=0.000013  Steps=41881\n",
            "[Episode 42200] AvgScore=166.0  Loss=0.0119  Eps=0.0150  LR=0.000013  Steps=41981\n",
            "[Episode 42300] AvgScore=172.9  Loss=0.0119  Eps=0.0149  LR=0.000013  Steps=42081\n",
            "[Episode 42400] AvgScore=171.0  Loss=0.0121  Eps=0.0147  LR=0.000013  Steps=42181\n",
            "[Episode 42500] AvgScore=165.6  Loss=0.0122  Eps=0.0146  LR=0.000013  Steps=42281\n",
            "[Episode 42600] AvgScore=170.3  Loss=0.0121  Eps=0.0144  LR=0.000013  Steps=42381\n",
            "[Episode 42700] AvgScore=167.4  Loss=0.0122  Eps=0.0143  LR=0.000013  Steps=42481\n",
            "[Episode 42800] AvgScore=170.2  Loss=0.0122  Eps=0.0141  LR=0.000013  Steps=42581\n",
            "[Episode 42900] AvgScore=168.1  Loss=0.0120  Eps=0.0140  LR=0.000013  Steps=42681\n",
            "[Episode 43000] AvgScore=163.7  Loss=0.0123  Eps=0.0139  LR=0.000013  Steps=42781\n",
            "Checkpoint saved at episode 43000\n",
            "[Episode 43100] AvgScore=168.7  Loss=0.0124  Eps=0.0137  LR=0.000013  Steps=42881\n",
            "[Episode 43200] AvgScore=167.5  Loss=0.0125  Eps=0.0136  LR=0.000013  Steps=42981\n",
            "[Episode 43300] AvgScore=164.7  Loss=0.0123  Eps=0.0135  LR=0.000013  Steps=43081\n",
            "[Episode 43400] AvgScore=171.1  Loss=0.0123  Eps=0.0133  LR=0.000013  Steps=43181\n",
            "[Episode 43500] AvgScore=170.4  Loss=0.0121  Eps=0.0132  LR=0.000013  Steps=43281\n",
            "[Episode 43600] AvgScore=170.6  Loss=0.0121  Eps=0.0131  LR=0.000013  Steps=43381\n",
            "[Episode 43700] AvgScore=169.7  Loss=0.0119  Eps=0.0129  LR=0.000013  Steps=43481\n",
            "[Episode 43800] AvgScore=168.8  Loss=0.0119  Eps=0.0128  LR=0.000013  Steps=43581\n",
            "[Episode 43900] AvgScore=164.9  Loss=0.0119  Eps=0.0127  LR=0.000013  Steps=43681\n",
            "[Episode 44000] AvgScore=168.7  Loss=0.0118  Eps=0.0125  LR=0.000013  Steps=43781\n",
            "Checkpoint saved at episode 44000\n",
            "[Episode 44100] AvgScore=165.3  Loss=0.0118  Eps=0.0124  LR=0.000013  Steps=43881\n",
            "[Episode 44200] AvgScore=167.3  Loss=0.0119  Eps=0.0123  LR=0.000013  Steps=43981\n",
            "[Episode 44300] AvgScore=168.0  Loss=0.0119  Eps=0.0122  LR=0.000013  Steps=44081\n",
            "[Episode 44400] AvgScore=166.3  Loss=0.0120  Eps=0.0121  LR=0.000013  Steps=44181\n",
            "[Episode 44500] AvgScore=164.9  Loss=0.0121  Eps=0.0119  LR=0.000013  Steps=44281\n",
            "[Episode 44600] AvgScore=168.1  Loss=0.0121  Eps=0.0118  LR=0.000013  Steps=44381\n",
            "[Episode 44700] AvgScore=172.7  Loss=0.0118  Eps=0.0117  LR=0.000013  Steps=44481\n",
            "[Episode 44800] AvgScore=169.8  Loss=0.0118  Eps=0.0116  LR=0.000013  Steps=44581\n",
            "[Episode 44900] AvgScore=168.8  Loss=0.0117  Eps=0.0115  LR=0.000013  Steps=44681\n",
            "[Episode 45000] AvgScore=169.4  Loss=0.0115  Eps=0.0114  LR=0.000013  Steps=44781\n",
            "Checkpoint saved at episode 45000\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "agent = train_agent(\n",
        "    episodes=50000,\n",
        "    save_path='yahtzee_model.pth'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goCJT10V-3A9",
        "outputId": "de2bc1c9-f174-4090-c280-6e6beff534cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading checkpoint from /content/checkpoint.pth...\n",
            "Loaded 43000 existing log entries from CSV\n",
            "Resuming training from episode 43000\n",
            "Current epsilon: 0.0139\n",
            "Current steps: 42781\n",
            "Episodes remaining: 7000\n",
            "\n",
            "[Episode 43100] AvgScore=166.0  Loss=0.0000  Eps=0.0139  LR=0.000013  Steps=42781\n",
            "[Episode 43200] AvgScore=165.1  Loss=0.0000  Eps=0.0139  LR=0.000013  Steps=42781\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2815751108.py:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  states = torch.FloatTensor([e[0] for e in experiences]).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Episode 43300] AvgScore=167.4  Loss=0.0155  Eps=0.0137  LR=0.000013  Steps=42871\n",
            "[Episode 43400] AvgScore=169.8  Loss=0.0143  Eps=0.0136  LR=0.000013  Steps=42971\n",
            "[Episode 43500] AvgScore=163.3  Loss=0.0134  Eps=0.0135  LR=0.000013  Steps=43071\n",
            "[Episode 43600] AvgScore=166.4  Loss=0.0129  Eps=0.0133  LR=0.000013  Steps=43171\n",
            "[Episode 43700] AvgScore=163.2  Loss=0.0125  Eps=0.0132  LR=0.000013  Steps=43271\n",
            "[Episode 43800] AvgScore=169.7  Loss=0.0117  Eps=0.0131  LR=0.000013  Steps=43371\n",
            "[Episode 43900] AvgScore=170.0  Loss=0.0112  Eps=0.0129  LR=0.000013  Steps=43471\n",
            "[Episode 44000] AvgScore=172.4  Loss=0.0111  Eps=0.0128  LR=0.000013  Steps=43571\n",
            "✓ Checkpoint saved at episode 44000\n",
            "[Episode 44100] AvgScore=170.0  Loss=0.0111  Eps=0.0127  LR=0.000013  Steps=43671\n",
            "[Episode 44200] AvgScore=167.3  Loss=0.0111  Eps=0.0126  LR=0.000013  Steps=43771\n",
            "[Episode 44300] AvgScore=172.4  Loss=0.0110  Eps=0.0124  LR=0.000013  Steps=43871\n",
            "[Episode 44400] AvgScore=168.0  Loss=0.0111  Eps=0.0123  LR=0.000013  Steps=43971\n",
            "[Episode 44500] AvgScore=167.6  Loss=0.0111  Eps=0.0122  LR=0.000013  Steps=44071\n",
            "[Episode 44600] AvgScore=170.5  Loss=0.0111  Eps=0.0121  LR=0.000013  Steps=44171\n",
            "[Episode 44700] AvgScore=170.6  Loss=0.0111  Eps=0.0119  LR=0.000013  Steps=44271\n",
            "[Episode 44800] AvgScore=168.0  Loss=0.0113  Eps=0.0118  LR=0.000013  Steps=44371\n",
            "[Episode 44900] AvgScore=173.8  Loss=0.0112  Eps=0.0117  LR=0.000013  Steps=44471\n",
            "[Episode 45000] AvgScore=167.3  Loss=0.0111  Eps=0.0116  LR=0.000013  Steps=44571\n",
            "✓ Checkpoint saved at episode 45000\n",
            "[Episode 45100] AvgScore=167.0  Loss=0.0111  Eps=0.0115  LR=0.000013  Steps=44671\n",
            "[Episode 45200] AvgScore=170.4  Loss=0.0110  Eps=0.0114  LR=0.000013  Steps=44771\n",
            "[Episode 45300] AvgScore=169.8  Loss=0.0108  Eps=0.0113  LR=0.000013  Steps=44871\n",
            "[Episode 45400] AvgScore=167.5  Loss=0.0108  Eps=0.0111  LR=0.000013  Steps=44971\n",
            "[Episode 45500] AvgScore=170.3  Loss=0.0105  Eps=0.0110  LR=0.000013  Steps=45071\n",
            "[Episode 45600] AvgScore=168.8  Loss=0.0102  Eps=0.0109  LR=0.000013  Steps=45171\n",
            "[Episode 45700] AvgScore=164.1  Loss=0.0104  Eps=0.0108  LR=0.000013  Steps=45271\n",
            "[Episode 45800] AvgScore=172.1  Loss=0.0104  Eps=0.0107  LR=0.000013  Steps=45371\n",
            "[Episode 45900] AvgScore=169.2  Loss=0.0102  Eps=0.0106  LR=0.000013  Steps=45471\n",
            "[Episode 46000] AvgScore=170.9  Loss=0.0102  Eps=0.0105  LR=0.000013  Steps=45571\n",
            "✓ Checkpoint saved at episode 46000\n",
            "[Episode 46100] AvgScore=165.6  Loss=0.0103  Eps=0.0104  LR=0.000013  Steps=45671\n",
            "[Episode 46200] AvgScore=169.9  Loss=0.0101  Eps=0.0103  LR=0.000013  Steps=45771\n",
            "[Episode 46300] AvgScore=167.2  Loss=0.0101  Eps=0.0102  LR=0.000013  Steps=45871\n",
            "[Episode 46400] AvgScore=172.2  Loss=0.0103  Eps=0.0101  LR=0.000013  Steps=45971\n",
            "[Episode 46500] AvgScore=170.7  Loss=0.0104  Eps=0.0100  LR=0.000013  Steps=46071\n",
            "[Episode 46600] AvgScore=169.8  Loss=0.0104  Eps=0.0100  LR=0.000013  Steps=46171\n",
            "[Episode 46700] AvgScore=171.4  Loss=0.0104  Eps=0.0100  LR=0.000013  Steps=46271\n",
            "[Episode 46800] AvgScore=170.3  Loss=0.0105  Eps=0.0100  LR=0.000013  Steps=46371\n",
            "[Episode 46900] AvgScore=170.1  Loss=0.0102  Eps=0.0100  LR=0.000013  Steps=46471\n",
            "[Episode 47000] AvgScore=168.9  Loss=0.0100  Eps=0.0100  LR=0.000013  Steps=46571\n",
            "✓ Checkpoint saved at episode 47000\n",
            "[Episode 47100] AvgScore=173.1  Loss=0.0099  Eps=0.0100  LR=0.000013  Steps=46671\n",
            "[Episode 47200] AvgScore=167.5  Loss=0.0099  Eps=0.0100  LR=0.000013  Steps=46771\n",
            "[Episode 47300] AvgScore=169.6  Loss=0.0096  Eps=0.0100  LR=0.000013  Steps=46871\n",
            "[Episode 47400] AvgScore=163.3  Loss=0.0097  Eps=0.0100  LR=0.000013  Steps=46971\n",
            "[Episode 47500] AvgScore=168.3  Loss=0.0098  Eps=0.0100  LR=0.000013  Steps=47071\n",
            "[Episode 47600] AvgScore=170.3  Loss=0.0097  Eps=0.0100  LR=0.000013  Steps=47171\n",
            "[Episode 47700] AvgScore=167.2  Loss=0.0096  Eps=0.0100  LR=0.000013  Steps=47271\n",
            "[Episode 47800] AvgScore=167.9  Loss=0.0098  Eps=0.0100  LR=0.000013  Steps=47371\n",
            "[Episode 47900] AvgScore=165.9  Loss=0.0097  Eps=0.0100  LR=0.000013  Steps=47471\n",
            "[Episode 48000] AvgScore=168.4  Loss=0.0097  Eps=0.0100  LR=0.000013  Steps=47571\n",
            "✓ Checkpoint saved at episode 48000\n",
            "[Episode 48100] AvgScore=172.5  Loss=0.0097  Eps=0.0100  LR=0.000013  Steps=47671\n",
            "[Episode 48200] AvgScore=175.9  Loss=0.0098  Eps=0.0100  LR=0.000013  Steps=47771\n",
            "[Episode 48300] AvgScore=169.9  Loss=0.0097  Eps=0.0100  LR=0.000013  Steps=47871\n",
            "[Episode 48400] AvgScore=170.5  Loss=0.0097  Eps=0.0100  LR=0.000013  Steps=47971\n",
            "[Episode 48500] AvgScore=170.5  Loss=0.0097  Eps=0.0100  LR=0.000013  Steps=48071\n",
            "[Episode 48600] AvgScore=168.0  Loss=0.0096  Eps=0.0100  LR=0.000013  Steps=48171\n",
            "[Episode 48700] AvgScore=169.5  Loss=0.0094  Eps=0.0100  LR=0.000013  Steps=48271\n",
            "[Episode 48800] AvgScore=169.8  Loss=0.0095  Eps=0.0100  LR=0.000013  Steps=48371\n",
            "[Episode 48900] AvgScore=172.8  Loss=0.0096  Eps=0.0100  LR=0.000013  Steps=48471\n",
            "[Episode 49000] AvgScore=168.1  Loss=0.0094  Eps=0.0100  LR=0.000013  Steps=48571\n",
            "✓ Checkpoint saved at episode 49000\n",
            "[Episode 49100] AvgScore=172.1  Loss=0.0095  Eps=0.0100  LR=0.000013  Steps=48671\n",
            "[Episode 49200] AvgScore=167.0  Loss=0.0097  Eps=0.0100  LR=0.000013  Steps=48771\n",
            "[Episode 49300] AvgScore=173.6  Loss=0.0097  Eps=0.0100  LR=0.000013  Steps=48871\n",
            "[Episode 49400] AvgScore=164.9  Loss=0.0096  Eps=0.0100  LR=0.000013  Steps=48971\n",
            "[Episode 49500] AvgScore=166.9  Loss=0.0097  Eps=0.0100  LR=0.000013  Steps=49071\n",
            "[Episode 49600] AvgScore=171.1  Loss=0.0097  Eps=0.0100  LR=0.000013  Steps=49171\n",
            "[Episode 49700] AvgScore=172.1  Loss=0.0097  Eps=0.0100  LR=0.000013  Steps=49271\n",
            "[Episode 49800] AvgScore=165.4  Loss=0.0096  Eps=0.0100  LR=0.000013  Steps=49371\n",
            "[Episode 49900] AvgScore=167.2  Loss=0.0094  Eps=0.0100  LR=0.000013  Steps=49471\n",
            "[Episode 50000] AvgScore=169.0  Loss=0.0094  Eps=0.0100  LR=0.000013  Steps=49571\n",
            "✓ Checkpoint saved at episode 50000\n",
            "Model saved to yahtzee_model_final.pth\n",
            "\n",
            "✓ Training complete!\n",
            "✓ CSV saved to /content/training_log.csv\n",
            "✓ Final model saved to yahtzee_model_final.pth\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def resume_training(checkpoint_path, csv_path, total_episodes=50000, checkpoint_interval=1000):\n",
        "    \"\"\"\n",
        "    Resume training from a saved checkpoint.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path: Path to the .pth checkpoint file\n",
        "        csv_path: Path to the training_log.csv file\n",
        "        total_episodes: Total number of episodes to train to (default 50000)\n",
        "        checkpoint_interval: How often to save checkpoints (default 1000)\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    checkpoint_path = '/content/checkpoint.pth'\n",
        "    csv_path = '/content/training_log.csv'\n",
        "    # Initialize game and agent\n",
        "    game = YahtzeeGame()\n",
        "    state_size = len(game.get_state())\n",
        "    action_size = 32 + 13\n",
        "    agent = YahtzeeAgent(state_size, action_size)\n",
        "\n",
        "    # Load checkpoint (weights_only=False for compatibility with numpy objects)\n",
        "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "\n",
        "    # Load agent state\n",
        "    agent.q_network.load_state_dict(checkpoint['q_network'])\n",
        "    agent.target_network.load_state_dict(checkpoint['target_network'])\n",
        "    agent.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    agent.scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "    agent.epsilon = checkpoint['epsilon']\n",
        "    agent.steps = checkpoint['steps']\n",
        "    agent.training_metrics = checkpoint.get('training_metrics', agent.training_metrics)\n",
        "\n",
        "    # Load training progress\n",
        "    start_episode = checkpoint.get('episode', 0)\n",
        "\n",
        "    # Load existing log rows from CSV\n",
        "    log_rows = []\n",
        "    if os.path.exists(csv_path):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        log_rows = df.to_dict('records')\n",
        "        print(f\"Loaded {len(log_rows)} existing log entries from CSV\")\n",
        "    else:\n",
        "        log_rows = checkpoint.get('log_rows', [])\n",
        "        print(f\"Loaded {len(log_rows)} log entries from checkpoint\")\n",
        "\n",
        "    print(f\"Resuming training from episode {start_episode}\")\n",
        "    print(f\"Current epsilon: {agent.epsilon:.4f}\")\n",
        "    print(f\"Current steps: {agent.steps}\")\n",
        "    print(f\"Episodes remaining: {total_episodes - start_episode}\\n\")\n",
        "\n",
        "    recent_scores = deque(maxlen=100)\n",
        "    losses = []\n",
        "\n",
        "    # Continue training\n",
        "    for episode in range(start_episode, total_episodes):\n",
        "\n",
        "        score, reward, steps_taken = play_episode(game, agent, training=True)\n",
        "        recent_scores.append(score)\n",
        "\n",
        "        beta = min(1.0, 0.4 + 0.6 * episode / total_episodes)\n",
        "        loss = agent.train_step(beta)\n",
        "        if loss > 0:\n",
        "            losses.append(loss)\n",
        "\n",
        "        agent.scheduler.step()\n",
        "\n",
        "        # Progress updates\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_score = np.mean(recent_scores)\n",
        "            avg_loss = (np.mean(losses[-500:])\n",
        "                        if len(losses) >= 500\n",
        "                        else (np.mean(losses) if losses else 0))\n",
        "            lr = agent.optimizer.param_groups[0]['lr']\n",
        "\n",
        "            print(f\"[Episode {episode+1}] \"\n",
        "                  f\"AvgScore={avg_score:.1f}  \"\n",
        "                  f\"Loss={avg_loss:.4f}  \"\n",
        "                  f\"Eps={agent.epsilon:.4f}  \"\n",
        "                  f\"LR={lr:.6f}  \"\n",
        "                  f\"Steps={agent.steps}\")\n",
        "\n",
        "        # Log this episode\n",
        "        lr = agent.optimizer.param_groups[0]['lr']\n",
        "        log_rows.append({\n",
        "            'episode': episode + 1,\n",
        "            'score': score,\n",
        "            'reward': reward,\n",
        "            'steps': steps_taken,\n",
        "            'epsilon': agent.epsilon,\n",
        "            'learning_rate': lr,\n",
        "            'loss': float(loss if loss > 0 else 0.0)\n",
        "        })\n",
        "\n",
        "        # Save checkpoint periodically\n",
        "        if (episode + 1) % checkpoint_interval == 0:\n",
        "            checkpoint = {\n",
        "                'q_network': agent.q_network.state_dict(),\n",
        "                'target_network': agent.target_network.state_dict(),\n",
        "                'optimizer': agent.optimizer.state_dict(),\n",
        "                'scheduler': agent.scheduler.state_dict(),\n",
        "                'epsilon': agent.epsilon,\n",
        "                'steps': agent.steps,\n",
        "                'training_metrics': agent.training_metrics,\n",
        "                'episode': episode + 1,\n",
        "                'log_rows': log_rows\n",
        "            }\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "            # Save CSV incrementally\n",
        "            with open(csv_path, 'w', newline='') as f:\n",
        "                writer = csv.DictWriter(f, fieldnames=list(log_rows[0].keys()))\n",
        "                writer.writeheader()\n",
        "                writer.writerows(log_rows)\n",
        "\n",
        "            print(f\"✓ Checkpoint saved at episode {episode+1}\")\n",
        "\n",
        "    # Save final model\n",
        "    final_model_path = 'yahtzee_model_final.pth'\n",
        "    agent.save(final_model_path)\n",
        "\n",
        "    # Write final CSV\n",
        "    with open(csv_path, 'w', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=list(log_rows[0].keys()))\n",
        "        writer.writeheader()\n",
        "        writer.writerows(log_rows)\n",
        "\n",
        "    print(f\"\\n✓ Training complete!\")\n",
        "    print(f\"✓ CSV saved to {csv_path}\")\n",
        "    print(f\"✓ Final model saved to {final_model_path}\\n\")\n",
        "\n",
        "    return agent\n",
        "\n",
        "\n",
        "# Usage example:\n",
        "# Resume from episode 43000 and continue to 50000\n",
        "agent = resume_training(\n",
        "    checkpoint_path=checkpoint_path,  # '/content/drive/MyDrive/Yathzee/checkpoint.pth'\n",
        "    csv_path='training_log.csv',\n",
        "    total_episodes=50000,\n",
        "    checkpoint_interval=1000\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKld1ywaQedu"
      },
      "source": [
        "## Test the Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvShJTG1Qedu",
        "outputId": "96478e93-d5ff-4c66-d38f-804f376e91b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== TEST RESULTS (100 games) ===\n",
            "Average: 170.8\n",
            "Std Dev: 29.2\n",
            "Median: 172.5\n",
            "Min: 103\n",
            "Max: 228\n",
            ">200: 18/100\n",
            ">250: 0/100\n"
          ]
        }
      ],
      "source": [
        "# Test\n",
        "test_scores = test_agent(agent, num_games=100, verbose=False)\n",
        "\n",
        "# Save training results for external plotting\n",
        "with open(\"test_scores.csv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"game\", \"score\"])\n",
        "    for i, s in enumerate(test_scores):\n",
        "        writer.writerow([i, s])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
