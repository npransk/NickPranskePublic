{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O480WZvbQedp"
      },
      "source": [
        "# Yahtzee Deep Q-Learning Training\n",
        "\n",
        "This notebook implements a Dueling DQN agent to play Yahtzee using prioritized experience replay."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga4z3AEfQedq"
      },
      "source": [
        "## Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKqu4gJrEr83",
        "outputId": "548386f9-9e67-40ad-b763-c2ecf29060f7"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# save_path = '/content/drive'\n",
        "# checkpoint_path = '/content/drive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1Q37aBLQedq",
        "outputId": "59328d7b-563f-4675-e1e2-e71bfb9a1ed3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "import os\n",
        "\n",
        "\"\"\"\n",
        "COMPUTE DEVICE SETUP:\n",
        "This whole program would not have been possible if it weren't for cloud-based GPUs.\n",
        "More details in the READMEs, but I ran this on an NVIDIA L4 GPU via a Google Colab notebook.\n",
        "\"\"\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Define random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "# Define Yahtzee categories\n",
        "CATEGORIES = [\n",
        "    'ones', 'twos', 'threes', 'fours', 'fives', 'sixes',\n",
        "    'three_of_kind', 'four_of_kind', 'full_house',\n",
        "    'small_straight', 'large_straight', 'yahtzee', 'chance'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9RIJ9TwQedq"
      },
      "source": [
        "## Yahtzee Game Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teU-H9FoQedr"
      },
      "outputs": [],
      "source": [
        "# Create Yahtzee gameplay env\n",
        "class YahtzeeGame:\n",
        "\n",
        "    # The next few functions are to reset the state, this first one is to automatically do that at the start\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.dice = np.zeros(5, dtype=int)\n",
        "        self.scorecard = {cat: None for cat in CATEGORIES}\n",
        "        self.roll_count = 0\n",
        "        self.turn = 0\n",
        "        self.total_score = 0\n",
        "        self.yahtzee_bonus_count = 0\n",
        "        self.first_roll_of_turn = True\n",
        "        self.game_log = []  # Track for analysis\n",
        "        return self.get_state()\n",
        "\n",
        "    # Define the action to roll the dice; this is done up to 3 times per turn\n",
        "    def roll_dice(self, keep_mask=None):\n",
        "\n",
        "        if keep_mask is None:\n",
        "            keep_mask = [0, 0, 0, 0, 0]\n",
        "\n",
        "        for i in range(5):\n",
        "            if not keep_mask[i]:\n",
        "                self.dice[i] = random.randint(1, 6)\n",
        "\n",
        "        self.roll_count += 1\n",
        "        self.first_roll_of_turn = False\n",
        "        self.game_log.append(('roll', keep_mask, self.dice.copy()))\n",
        "        return self.dice.copy()\n",
        "\n",
        "    # In order for Q-learning to work, we must calculate what the current dice would score in a given category\n",
        "    def calculate_score(self, category):\n",
        "        dice = self.dice\n",
        "        counts = np.bincount(dice, minlength=7)[1:]\n",
        "\n",
        "        # Define scoring rules for each category\n",
        "        if category == 'ones':\n",
        "            return np.sum(dice == 1)\n",
        "        elif category == 'twos':\n",
        "            return np.sum(dice == 2) * 2\n",
        "        elif category == 'threes':\n",
        "            return np.sum(dice == 3) * 3\n",
        "        elif category == 'fours':\n",
        "            return np.sum(dice == 4) * 4\n",
        "        elif category == 'fives':\n",
        "            return np.sum(dice == 5) * 5\n",
        "        elif category == 'sixes':\n",
        "            return np.sum(dice == 6) * 6\n",
        "        elif category == 'three_of_kind':\n",
        "            return np.sum(dice) if np.max(counts) >= 3 else 0\n",
        "        elif category == 'four_of_kind':\n",
        "            return np.sum(dice) if np.max(counts) >= 4 else 0\n",
        "        elif category == 'full_house':\n",
        "            return 25 if sorted(counts[counts > 0]) == [2, 3] else 0\n",
        "        elif category == 'small_straight':\n",
        "            dice_set = set(dice)\n",
        "            straights = [{1,2,3,4}, {2,3,4,5}, {3,4,5,6}]\n",
        "            return 30 if any(s.issubset(dice_set) for s in straights) else 0\n",
        "        elif category == 'large_straight':\n",
        "            return 40 if set(dice) in [{1,2,3,4,5}, {2,3,4,5,6}] else 0\n",
        "        elif category == 'yahtzee':\n",
        "            return 50 if np.max(counts) == 5 else 0\n",
        "        elif category == 'chance':\n",
        "            return np.sum(dice)\n",
        "        return 0\n",
        "\n",
        "    # Using the above parameters for each category, we can now score the current dice in a given category\n",
        "    # Returns -1 if invalid move\n",
        "    def score_category(self, category):\n",
        "        if self.scorecard[category] is not None:\n",
        "            return -1  # Category already used\n",
        "        if self.first_roll_of_turn:\n",
        "            return -1  # Must roll before scoring\n",
        "\n",
        "        score = self.calculate_score(category)\n",
        "\n",
        "        # Yahtzee bonus: +100 for each additional Yahtzee after first\n",
        "        is_yahtzee = np.max(np.bincount(self.dice, minlength=7)[1:]) == 5\n",
        "        if is_yahtzee and self.scorecard['yahtzee'] is not None and self.scorecard['yahtzee'] > 0:\n",
        "            score += 100\n",
        "            self.yahtzee_bonus_count += 1\n",
        "\n",
        "        self.scorecard[category] = score\n",
        "        self.total_score += score\n",
        "        self.game_log.append(('score', category, score))\n",
        "\n",
        "        # Upper section bonus at game end (+35 if 63+ points)\n",
        "        if self.is_game_over():\n",
        "            upper_score = sum(self.scorecard[cat] for cat in CATEGORIES[:6] if self.scorecard[cat] is not None)\n",
        "            if upper_score >= 63:\n",
        "                self.total_score += 35\n",
        "                self.game_log.append(('bonus', 'upper', 35))\n",
        "\n",
        "        # Reset for next turn\n",
        "        self.turn += 1\n",
        "        self.roll_count = 0\n",
        "        self.first_roll_of_turn = True\n",
        "\n",
        "        return score\n",
        "\n",
        "    def is_game_over(self):\n",
        "        return all(score is not None for score in self.scorecard.values())\n",
        "\n",
        "    def get_state(self):\n",
        "        # Get current game state as feature vector.\n",
        "\n",
        "        # Dice one-hot encoding (see READMEs for explanation) - basically just a way to turn categories into numbers\n",
        "        dice_onehot = np.zeros(30)\n",
        "        for i, die in enumerate(self.dice):\n",
        "            if die > 0:\n",
        "                dice_onehot[i * 6 + die - 1] = 1\n",
        "\n",
        "        # Scorecard status\n",
        "        scorecard_filled = np.array([1.0 if self.scorecard[cat] is not None else 0.0 for cat in CATEGORIES])\n",
        "\n",
        "        \"\"\"\n",
        "        I like this next part so I want to explain it here:\n",
        "        The program will create an array of placeholders for each category, then for each category\n",
        "        that is still available it will calculate what the current roll would score. So let's sat that\n",
        "        you roll a [2,2,5,5,6]. It will calculate the categories as this:\n",
        "        [0, 4, 0, 0, 10, 6, 0, 0, 0, 0, 0, 0, 20]\n",
        "\n",
        "        But then it will normalize it to the highest normal score in Yahtzee (50):\n",
        "        [0, 0.08, 0, 0, 0.2, 0.12, 0, 0, 0, 0, 0, 0, 0.4]\n",
        "\n",
        "        and use that to figure out which categories are most valuable to go for with the current roll!\n",
        "        \"\"\"\n",
        "\n",
        "        # Potential scores (normalized)\n",
        "        potential_scores = np.zeros(13) # placeholders for each category\n",
        "        if not self.first_roll_of_turn: # Don't need to calculate if no roll yet\n",
        "            for i, cat in enumerate(CATEGORIES):\n",
        "                if self.scorecard[cat] is None: # if category is still available\n",
        "                    potential_scores[i] = self.calculate_score(cat) / 50.0 # Normalize to between 0 and 1\n",
        "\n",
        "        # Upper section progress (critical for 63-point bonus strategy)\n",
        "        upper_scores = np.array([\n",
        "            (self.scorecard[cat] if self.scorecard[cat] is not None else 0) / 18.0\n",
        "            for cat in CATEGORIES[:6]\n",
        "        ])\n",
        "\n",
        "        # Turn and roll info\n",
        "        roll_count = np.array([self.roll_count / 3.0])\n",
        "        turn_progress = np.array([self.turn / 13.0])\n",
        "        upper_sum = sum(self.scorecard[cat] if self.scorecard[cat] is not None else 0\n",
        "                       for cat in CATEGORIES[:6])\n",
        "        upper_progress = np.array([min(upper_sum / 63.0, 1.5)])  # Can exceed 1.0\n",
        "        has_rolled = np.array([0.0 if self.first_roll_of_turn else 1.0])\n",
        "        score_normalized = np.array([self.total_score / 400.0])\n",
        "        yahtzee_bonuses = np.array([self.yahtzee_bonus_count / 3.0])\n",
        "\n",
        "        # return the features the model is going to evaulate\n",
        "        return np.concatenate([\n",
        "            dice_onehot,           # 30\n",
        "            scorecard_filled,      # 13\n",
        "            potential_scores,      # 13\n",
        "            upper_scores,          # 6\n",
        "            roll_count,            # 1\n",
        "            turn_progress,         # 1\n",
        "            upper_progress,        # 1\n",
        "            has_rolled,            # 1\n",
        "            score_normalized,      # 1\n",
        "            yahtzee_bonuses        # 1\n",
        "        ])\n",
        "\n",
        "    def get_valid_actions(self):\n",
        "        #n Get all valid actions from current state\n",
        "        actions = []\n",
        "\n",
        "        # Must roll at start of turn\n",
        "        if self.first_roll_of_turn:\n",
        "            actions.append(('roll', tuple([0, 0, 0, 0, 0])))\n",
        "            return actions\n",
        "\n",
        "        # Can roll if under 3 rolls this turn\n",
        "        if self.roll_count < 3:\n",
        "            for i in range(32):\n",
        "                keep_mask = tuple((i >> j) & 1 for j in range(5))\n",
        "                actions.append(('roll', keep_mask))\n",
        "\n",
        "        # Can score in any available category\n",
        "        for cat in CATEGORIES:\n",
        "            if self.scorecard[cat] is None:\n",
        "                actions.append(('score', cat))\n",
        "\n",
        "        return actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5rEBl3SQeds"
      },
      "source": [
        "## Neural Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hURPgGYtQeds"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This is where most of the magic happens. An explanation of Dueling DQN is in the READMEs\n",
        "\"\"\"\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=512):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "\n",
        "        self.feature = nn.Sequential( # shared network\n",
        "            nn.Linear(state_size, hidden_size), # fully connected layers\n",
        "            nn.ReLU(), # best activation function, according to my professors in grad school\n",
        "            nn.LayerNorm(hidden_size), # the internet told me to add this\n",
        "            nn.Dropout(0.1), # apparently reduces overfitting\n",
        "            ### Repeat for hidden layers\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.Dropout(0.1),\n",
        "        )\n",
        "\n",
        "        # Value stream: estimates state value\n",
        "        # V(s) = how good the state is\n",
        "        self.value = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 2, 1)\n",
        "        )\n",
        "\n",
        "        # Advantage stream: estimates action advantages\n",
        "        # This is the benefit of Dueling DQN over Q-learning - it separates state value from action advantage\n",
        "        self.advantage = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 2, action_size)\n",
        "        )\n",
        "\n",
        "    # Dueling architecture combines streams to get Q-values\n",
        "    def forward(self, x):\n",
        "        features = self.feature(x)\n",
        "        value = self.value(features)\n",
        "        advantage = self.advantage(features)\n",
        "\n",
        "        # Combine: Q = V + (A - mean(A))\n",
        "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "        return q_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDhcw3_rQeds"
      },
      "source": [
        "## Prioritized Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciyWcW1oQeds"
      },
      "outputs": [],
      "source": [
        "class PrioritizedReplayBuffer:\n",
        "\n",
        "    # Initialize buffer with memory capacity and prioritization alpha (how strongly to priorize high-error samples)\n",
        "    def __init__(self, capacity=300000, alpha=0.7): # High alpha so it can take risks to hopefully beat mathematical strategies\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.buffer = []\n",
        "        self.priorities = []\n",
        "        self.position = 0\n",
        "\n",
        "    # Add experiences to memory buffer\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        max_priority = max(self.priorities) if self.priorities else 1.0\n",
        "\n",
        "        experience = (state, action, reward, next_state, done)\n",
        "\n",
        "        # Replace old memory if at capacity\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(experience)\n",
        "            self.priorities.append(max_priority)\n",
        "        else:\n",
        "            self.buffer[self.position] = experience\n",
        "            self.priorities[self.position] = max_priority\n",
        "\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        if len(self.buffer) == 0:\n",
        "            return None\n",
        "\n",
        "        # Meat and potatoes of PER right here\n",
        "        priorities = np.array(self.priorities[:len(self.buffer)])\n",
        "        probs = priorities ** self.alpha\n",
        "        probs /= probs.sum()\n",
        "\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs, replace=False)\n",
        "\n",
        "        # Importance sampling weights\n",
        "        total = len(self.buffer)\n",
        "        weights = (total * probs[indices]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "\n",
        "        experiences = [self.buffer[idx] for idx in indices]\n",
        "\n",
        "        # Convert to tensors for PyTorch digestion\n",
        "        states = torch.FloatTensor([e[0] for e in experiences]).to(device)\n",
        "        actions = torch.LongTensor([e[1] for e in experiences]).to(device)\n",
        "        rewards = torch.FloatTensor([e[2] for e in experiences]).to(device)\n",
        "        next_states = torch.FloatTensor([e[3] for e in experiences]).to(device)\n",
        "        dones = torch.FloatTensor([e[4] for e in experiences]).to(device)\n",
        "        weights_tensor = torch.FloatTensor(weights).to(device)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones, weights_tensor, indices\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            self.priorities[idx] = priority\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZYjV3xBQeds"
      },
      "source": [
        "## DQN Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fUR16bSQeds"
      },
      "outputs": [],
      "source": [
        "class YahtzeeAgent:\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Initialize networks\n",
        "        self.q_network = DuelingDQN(state_size, action_size, hidden_size=512).to(device)\n",
        "        self.target_network = DuelingDQN(state_size, action_size, hidden_size=512).to(device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        # Optimizer with multi-step Learning Rate decay\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.0001)\n",
        "        self.scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "            self.optimizer,\n",
        "            milestones=[10000, 20000, 30000],  # Drop LR at these episodes\n",
        "            gamma=0.5  # Multiply LR by 0.5 at each milestone\n",
        "        )\n",
        "\n",
        "        # Replay buffer\n",
        "        self.memory = PrioritizedReplayBuffer(capacity=300000, alpha=0.6)\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.epsilon = 1.0 # Initial rate of random exploration\n",
        "        self.epsilon_min = 0.01 # Target minimum exploration rate\n",
        "        self.epsilon_decay = 0.9999  # Slow decay for better exploration\n",
        "        self.gamma = 0.99\n",
        "        self.batch_size = 256 # Could probably go higher, but I don't want to toast the GPUs\n",
        "        self.update_target_every = 500\n",
        "        self.train_start = 10000  # Warmup period\n",
        "        self.steps = 0\n",
        "\n",
        "        # Action mapping\n",
        "        self.action_map = self._create_action_map()\n",
        "\n",
        "        # Training metrics\n",
        "        self.training_metrics = {\n",
        "            'q_values': [],\n",
        "            'td_errors': [],\n",
        "            'gradient_norms': []\n",
        "        }\n",
        "\n",
        "    # Figure out every possible action and map it to an index - this was annoying to figure out\n",
        "    def _create_action_map(self):\n",
        "        action_map = []\n",
        "        for i in range(32):\n",
        "            keep_mask = tuple((i >> j) & 1 for j in range(5))\n",
        "            action_map.append(('roll', keep_mask))\n",
        "        for cat in CATEGORIES:\n",
        "            action_map.append(('score', cat))\n",
        "        return action_map\n",
        "\n",
        "    def get_action_index(self, action):\n",
        "        try:\n",
        "            return self.action_map.index(action)\n",
        "        except ValueError:\n",
        "            return 0\n",
        "\n",
        "    def select_action(self, state, valid_actions, training=True):\n",
        "        # Epsilon-greedy action selection\n",
        "        if training and random.random() < self.epsilon:\n",
        "            return random.choice(valid_actions)\n",
        "\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_values = self.q_network(state_tensor).cpu().numpy()[0]\n",
        "\n",
        "        # Mask invalid actions\n",
        "        valid_indices = [self.get_action_index(a) for a in valid_actions]\n",
        "        masked_q = np.full(self.action_size, -1e10)\n",
        "        masked_q[valid_indices] = q_values[valid_indices]\n",
        "\n",
        "        best_action_idx = np.argmax(masked_q)\n",
        "        return self.action_map[best_action_idx]\n",
        "\n",
        "    def train_step(self, beta=0.4):\n",
        "        # Single training step with prioritized experience replay\n",
        "        if len(self.memory) < self.train_start:\n",
        "            return 0.0\n",
        "\n",
        "        batch = self.memory.sample(self.batch_size, beta)\n",
        "        if batch is None:\n",
        "            return 0.0\n",
        "\n",
        "        states, actions, rewards, next_states, dones, weights, indices = batch\n",
        "\n",
        "        # Current Q values\n",
        "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "        # Double DQN target to prevent overestimation\n",
        "        with torch.no_grad():\n",
        "            next_actions = self.q_network(next_states).max(1)[1]\n",
        "            next_q = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
        "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
        "\n",
        "        # TD errors for priority updates\n",
        "        td_errors = torch.abs(current_q - target_q).detach().cpu().numpy()\n",
        "\n",
        "        # Weighted Huber loss (stablizes training)\n",
        "        loss = (weights * nn.SmoothL1Loss(reduction='none')(current_q, target_q)).mean()\n",
        "\n",
        "        # Optimize and implement gradient clipping\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update priorities (with small epsilon to avoid zero priority)\n",
        "        self.memory.update_priorities(indices, td_errors + 1e-6)\n",
        "\n",
        "        # Update target network\n",
        "        self.steps += 1\n",
        "        if self.steps % self.update_target_every == 0:\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        # Decay epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        # Track metrics\n",
        "        self.training_metrics['td_errors'].append(td_errors.mean())\n",
        "        self.training_metrics['gradient_norms'].append(grad_norm.item())\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    # Save and load model functions\n",
        "    def save(self, filepath):\n",
        "        torch.save({\n",
        "            'q_network': self.q_network.state_dict(),\n",
        "            'target_network': self.target_network.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'scheduler': self.scheduler.state_dict(),\n",
        "            'epsilon': self.epsilon,\n",
        "            'steps': self.steps,\n",
        "            'training_metrics': self.training_metrics\n",
        "        }, filepath)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "\n",
        "    def load(self, filepath):\n",
        "        checkpoint = torch.load(filepath, map_location=device)\n",
        "        self.q_network.load_state_dict(checkpoint['q_network'])\n",
        "        self.target_network.load_state_dict(checkpoint['target_network'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        self.scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "        self.epsilon = checkpoint['epsilon']\n",
        "        self.steps = checkpoint['steps']\n",
        "        self.training_metrics = checkpoint.get('training_metrics', self.training_metrics)\n",
        "        print(f\"Model loaded from {filepath}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnOYRlbHQedt"
      },
      "source": [
        "## Reward Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99JOgILAQedt"
      },
      "outputs": [],
      "source": [
        "\"\"\"This part is incredibly important - the reward function defines how the agent learns!\n",
        "I didn't know exactly how to do it at first, so I iterated by running it about 20% of the way several times.\n",
        "However, I eventually just asked ChatGPT to rewrite it to be accurate to the probability of Yahtzee and to\n",
        "reward the agent accordingly. The result is a much better reward function that leads to far superior play.\n",
        "\"\"\"\n",
        "def calculate_reward(game, action_type, score, prev_score, category_used):\n",
        "\n",
        "    # ---- 1. Reward for rolling ----\n",
        "    if action_type == 'roll':\n",
        "        return 0.0  # rolling shouldn't bias the value function\n",
        "\n",
        "    # ---- 2. If the score is invalid ----\n",
        "    if score < 0:\n",
        "        return -10.0  # keep penalty small but discouraging\n",
        "\n",
        "    reward = 0.0\n",
        "\n",
        "    # Category expected values (approx across optimal play)\n",
        "    EV = {\n",
        "        \"ones\":   2.8,\n",
        "        \"twos\":   5.6,\n",
        "        \"threes\": 8.5,\n",
        "        \"fours\":  11.3,\n",
        "        \"fives\":  14.0,\n",
        "        \"sixes\":  16.8,\n",
        "        \"three_of_kind\": 12,\n",
        "        \"four_of_kind\":  7,\n",
        "        \"full_house\": 9.2,\n",
        "        \"small_straight\": 10,\n",
        "        \"large_straight\": 7.5,\n",
        "        \"yahtzee\": 2.5,\n",
        "        \"chance\": 23\n",
        "    }\n",
        "\n",
        "    # Reward based on improvement over the EV for that category\n",
        "    expected = EV.get(category_used, 0)\n",
        "    reward += (score - expected) / 10.0\n",
        "    #       ^ normalization so the agent sees consistent gradients\n",
        "\n",
        "    if category_used in CATEGORIES[:6]:\n",
        "        upper_sum = sum(game.scorecard[cat] if game.scorecard[cat] is not None else 0\n",
        "                       for cat in CATEGORIES[:6])\n",
        "        before = upper_sum - score   # previous total\n",
        "        after  = upper_sum           # new total\n",
        "\n",
        "        # Reward progress normalized by 63-point target\n",
        "        upper_progress = (after - before) / 63.0\n",
        "        reward += upper_progress * 3.0  # strong encouragement\n",
        "\n",
        "    if score == 0:\n",
        "\n",
        "        # Zeroing Yahtzee or 1s is fine\n",
        "        if category_used in (\"yahtzee\", \"ones\"):\n",
        "            reward -= 0.1  # almost neutral\n",
        "\n",
        "        # Zeroing Chance early is bad\n",
        "        elif category_used == \"chance\":\n",
        "            reward -= 2.0\n",
        "\n",
        "        # Zeroing upper categories mid-game is costly\n",
        "        elif category_used in CATEGORIES[:6]:\n",
        "            reward -= 1.5\n",
        "\n",
        "        else:\n",
        "            reward -= 0.5  # reasonable penalty\n",
        "\n",
        "    # Add a small rarity reward (helpful early)\n",
        "    rarity_bonus = {\n",
        "        \"yahtzee\": 1.5,\n",
        "        \"large_straight\": 0.9,\n",
        "        \"small_straight\": 0.4,\n",
        "        \"full_house\": 0.15,\n",
        "    }\n",
        "\n",
        "    if category_used in rarity_bonus and score > 0:\n",
        "        reward += rarity_bonus[category_used]\n",
        "\n",
        "    if game.is_game_over():\n",
        "        total = game.total_score\n",
        "\n",
        "        # Nonlinear shaping—smooth and RL-friendly\n",
        "        if total >= 300:\n",
        "            reward += 20.0\n",
        "        elif total >= 250:\n",
        "            reward += 12.0\n",
        "        elif total >= 200:\n",
        "            reward += 6.0\n",
        "        elif total >= 150:\n",
        "            reward += 2.0\n",
        "        else:\n",
        "            reward -= 5.0\n",
        "\n",
        "\n",
        "    return reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u68YduqrQedu"
      },
      "source": [
        "## Training and Testing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv_c6ThtQedu"
      },
      "outputs": [],
      "source": [
        "def play_episode(game, agent, training=True, verbose=False):\n",
        "    # Play one complete Yahtzee game (this was fun to write)\n",
        "    state = game.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "\n",
        "    while not game.is_game_over() and steps < 250: # steps < 250 not needed, but a safety net for infinite loops\n",
        "        valid_actions = game.get_valid_actions()\n",
        "        if not valid_actions:\n",
        "            break\n",
        "\n",
        "        action = agent.select_action(state, valid_actions, training)\n",
        "        action_type, action_value = action\n",
        "\n",
        "        prev_score = game.total_score\n",
        "\n",
        "        if action_type == 'roll':\n",
        "            game.roll_dice(keep_mask=action_value)\n",
        "            score = 0\n",
        "            done = False\n",
        "            category_used = None\n",
        "        else:\n",
        "            score = game.score_category(action_value)\n",
        "            done = game.is_game_over()\n",
        "            category_used = action_value\n",
        "            if score < 0:\n",
        "                done = True\n",
        "\n",
        "        next_state = game.get_state()\n",
        "        reward = calculate_reward(game, action_type, score, prev_score, category_used)\n",
        "\n",
        "        if training:\n",
        "            action_idx = agent.get_action_index(action)\n",
        "            agent.memory.add(state, action_idx, reward, next_state, done)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return game.total_score, total_reward, steps\n",
        "\n",
        "\n",
        "def train_agent(episodes=50000, save_path='yahtzee_model.pth', checkpoint_interval=1000):\n",
        "\n",
        "    game = YahtzeeGame()\n",
        "    state_size = len(game.get_state())\n",
        "    action_size = 32 + 13\n",
        "\n",
        "    agent = YahtzeeAgent(state_size, action_size)\n",
        "\n",
        "    # Try to load existing checkpoint\n",
        "    checkpoint_path = 'checkpoint.pth'\n",
        "    start_episode = 0\n",
        "    log_rows = []\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        try:\n",
        "            checkpoint = torch.load(checkpoint_path)\n",
        "            # Load agent state\n",
        "            agent.q_network.load_state_dict(checkpoint['q_network'])\n",
        "            agent.target_network.load_state_dict(checkpoint['target_network'])\n",
        "            agent.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            agent.scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "            agent.epsilon = checkpoint['epsilon']\n",
        "            agent.steps = checkpoint['steps']\n",
        "            agent.training_metrics = checkpoint.get('training_metrics', agent.training_metrics)\n",
        "            # Load training progress\n",
        "            start_episode = checkpoint.get('episode', 0)\n",
        "            log_rows = checkpoint.get('log_rows', [])\n",
        "            print(f\"Resumed from episode {start_episode}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load checkpoint: {e}\")\n",
        "            print(\"Starting from scratch...\")\n",
        "\n",
        "    recent_scores = deque(maxlen=100)\n",
        "    losses = []\n",
        "\n",
        "    print(\"\\nStarting training...\\n\")\n",
        "\n",
        "    for episode in range(start_episode, episodes):\n",
        "\n",
        "        score, reward, steps_taken = play_episode(game, agent, training=True)\n",
        "        recent_scores.append(score)\n",
        "\n",
        "        beta = min(1.0, 0.4 + 0.6 * episode / episodes)\n",
        "        loss = agent.train_step(beta)\n",
        "        if loss > 0:\n",
        "            losses.append(loss)\n",
        "\n",
        "        agent.scheduler.step()\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_score = np.mean(recent_scores)\n",
        "            avg_loss = (np.mean(losses[-500:])\n",
        "                        if len(losses) >= 500\n",
        "                        else (np.mean(losses) if losses else 0))\n",
        "            lr = agent.optimizer.param_groups[0]['lr']\n",
        "\n",
        "            print(f\"[Episode {episode+1}] \"\n",
        "                  f\"AvgScore={avg_score:.1f}  \"\n",
        "                  f\"Loss={avg_loss:.4f}  \"\n",
        "                  f\"Eps={agent.epsilon:.4f}  \"\n",
        "                  f\"LR={lr:.6f}  \"\n",
        "                  f\"Steps={agent.steps}\")\n",
        "\n",
        "        lr = agent.optimizer.param_groups[0]['lr']\n",
        "        log_rows.append({\n",
        "            'episode': episode + 1,\n",
        "            'score': score,\n",
        "            'reward': reward,\n",
        "            'steps': steps_taken,\n",
        "            'epsilon': agent.epsilon,\n",
        "            'learning_rate': lr,\n",
        "            'loss': float(loss if loss > 0 else 0.0)\n",
        "        })\n",
        "\n",
        "        # Save checkpoint periodically\n",
        "        if (episode + 1) % checkpoint_interval == 0:\n",
        "            checkpoint = {\n",
        "                'q_network': agent.q_network.state_dict(),\n",
        "                'target_network': agent.target_network.state_dict(),\n",
        "                'optimizer': agent.optimizer.state_dict(),\n",
        "                'scheduler': agent.scheduler.state_dict(),\n",
        "                'epsilon': agent.epsilon,\n",
        "                'steps': agent.steps,\n",
        "                'training_metrics': agent.training_metrics,\n",
        "                'episode': episode + 1,\n",
        "                'log_rows': log_rows\n",
        "            }\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "            # Also save CSV incrementally\n",
        "            csv_path = \"training_log.csv\"\n",
        "            with open(csv_path, 'w', newline='') as f:\n",
        "                writer = csv.DictWriter(f, fieldnames=list(log_rows[0].keys()))\n",
        "                writer.writeheader()\n",
        "                writer.writerows(log_rows)\n",
        "\n",
        "            print(f\"Checkpoint saved at episode {episode+1}\")\n",
        "\n",
        "    # Save final model\n",
        "    agent.save(save_path)\n",
        "\n",
        "    # Write final CSV\n",
        "    csv_path = \"training_log.csv\"\n",
        "    with open(csv_path, 'w', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=list(log_rows[0].keys()))\n",
        "        writer.writeheader()\n",
        "        writer.writerows(log_rows)\n",
        "\n",
        "    print(f\"\\n✓ Training complete. CSV saved to {csv_path}\")\n",
        "    print(f\"✓ Model saved to {save_path}\\n\")\n",
        "\n",
        "    return agent\n",
        "\n",
        "\n",
        "def test_agent(agent, num_games=100, verbose=False):\n",
        "    # Run evaluation episodes with epsilon=0 and return scores.\n",
        "    game = YahtzeeGame()\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0\n",
        "\n",
        "    scores = []\n",
        "    rewards = []\n",
        "\n",
        "    for _ in range(num_games):\n",
        "        score, reward, _ = play_episode(game, agent, training=False, verbose=verbose)\n",
        "        scores.append(score)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    # Summary statistics\n",
        "    avg = np.mean(scores)\n",
        "    std = np.std(scores)\n",
        "    med = np.median(scores)\n",
        "\n",
        "    print(f\"\\n=== TEST RESULTS ({num_games} games) ===\")\n",
        "    print(f\"Average: {avg:.1f}\")\n",
        "    print(f\"Std Dev: {std:.1f}\")\n",
        "    print(f\"Median: {med:.1f}\")\n",
        "    print(f\"Min: {min(scores)}\")\n",
        "    print(f\"Max: {max(scores)}\")\n",
        "    print(f\">200: {sum(s >= 200 for s in scores)}/{num_games}\")\n",
        "    print(f\">250: {sum(s >= 250 for s in scores)}/{num_games}\")\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxf0EVIjQedu"
      },
      "source": [
        "## Train the Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A16g524gQedu",
        "outputId": "bf3819b6-e687-4c88-b2bb-ff74c515cdc1"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "agent = train_agent(\n",
        "    episodes=50000,\n",
        "    save_path='yahtzee_model.pth'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goCJT10V-3A9",
        "outputId": "de2bc1c9-f174-4090-c280-6e6beff534cd"
      },
      "outputs": [],
      "source": [
        "def resume_training(checkpoint_path, csv_path, total_episodes=50000, checkpoint_interval=1000):\n",
        "    \"\"\"\n",
        "    Resume training from a saved checkpoint.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path: Path to the .pth checkpoint file\n",
        "        csv_path: Path to the training_log.csv file\n",
        "        total_episodes: Total number of episodes to train to (default 50000)\n",
        "        checkpoint_interval: How often to save checkpoints (default 1000)\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    checkpoint_path = '/content/checkpoint.pth'\n",
        "    csv_path = '/content/training_log.csv'\n",
        "    # Initialize game and agent\n",
        "    game = YahtzeeGame()\n",
        "    state_size = len(game.get_state())\n",
        "    action_size = 32 + 13\n",
        "    agent = YahtzeeAgent(state_size, action_size)\n",
        "\n",
        "    # Load checkpoint (weights_only=False for compatibility with numpy objects)\n",
        "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "\n",
        "    # Load agent state\n",
        "    agent.q_network.load_state_dict(checkpoint['q_network'])\n",
        "    agent.target_network.load_state_dict(checkpoint['target_network'])\n",
        "    agent.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    agent.scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "    agent.epsilon = checkpoint['epsilon']\n",
        "    agent.steps = checkpoint['steps']\n",
        "    agent.training_metrics = checkpoint.get('training_metrics', agent.training_metrics)\n",
        "\n",
        "    # Load training progress\n",
        "    start_episode = checkpoint.get('episode', 0)\n",
        "\n",
        "    # Load existing log rows from CSV\n",
        "    log_rows = []\n",
        "    if os.path.exists(csv_path):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        log_rows = df.to_dict('records')\n",
        "        print(f\"Loaded {len(log_rows)} existing log entries from CSV\")\n",
        "    else:\n",
        "        log_rows = checkpoint.get('log_rows', [])\n",
        "        print(f\"Loaded {len(log_rows)} log entries from checkpoint\")\n",
        "\n",
        "    print(f\"Resuming training from episode {start_episode}\")\n",
        "    print(f\"Current epsilon: {agent.epsilon:.4f}\")\n",
        "    print(f\"Current steps: {agent.steps}\")\n",
        "    print(f\"Episodes remaining: {total_episodes - start_episode}\\n\")\n",
        "\n",
        "    recent_scores = deque(maxlen=100)\n",
        "    losses = []\n",
        "\n",
        "    # Continue training\n",
        "    for episode in range(start_episode, total_episodes):\n",
        "\n",
        "        score, reward, steps_taken = play_episode(game, agent, training=True)\n",
        "        recent_scores.append(score)\n",
        "\n",
        "        beta = min(1.0, 0.4 + 0.6 * episode / total_episodes)\n",
        "        loss = agent.train_step(beta)\n",
        "        if loss > 0:\n",
        "            losses.append(loss)\n",
        "\n",
        "        agent.scheduler.step()\n",
        "\n",
        "        # Progress updates\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_score = np.mean(recent_scores)\n",
        "            avg_loss = (np.mean(losses[-500:])\n",
        "                        if len(losses) >= 500\n",
        "                        else (np.mean(losses) if losses else 0))\n",
        "            lr = agent.optimizer.param_groups[0]['lr']\n",
        "\n",
        "            print(f\"[Episode {episode+1}] \"\n",
        "                  f\"AvgScore={avg_score:.1f}  \"\n",
        "                  f\"Loss={avg_loss:.4f}  \"\n",
        "                  f\"Eps={agent.epsilon:.4f}  \"\n",
        "                  f\"LR={lr:.6f}  \"\n",
        "                  f\"Steps={agent.steps}\")\n",
        "\n",
        "        # Log this episode\n",
        "        lr = agent.optimizer.param_groups[0]['lr']\n",
        "        log_rows.append({\n",
        "            'episode': episode + 1,\n",
        "            'score': score,\n",
        "            'reward': reward,\n",
        "            'steps': steps_taken,\n",
        "            'epsilon': agent.epsilon,\n",
        "            'learning_rate': lr,\n",
        "            'loss': float(loss if loss > 0 else 0.0)\n",
        "        })\n",
        "\n",
        "        # Save checkpoint periodically\n",
        "        if (episode + 1) % checkpoint_interval == 0:\n",
        "            checkpoint = {\n",
        "                'q_network': agent.q_network.state_dict(),\n",
        "                'target_network': agent.target_network.state_dict(),\n",
        "                'optimizer': agent.optimizer.state_dict(),\n",
        "                'scheduler': agent.scheduler.state_dict(),\n",
        "                'epsilon': agent.epsilon,\n",
        "                'steps': agent.steps,\n",
        "                'training_metrics': agent.training_metrics,\n",
        "                'episode': episode + 1,\n",
        "                'log_rows': log_rows\n",
        "            }\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "            # Save CSV incrementally\n",
        "            with open(csv_path, 'w', newline='') as f:\n",
        "                writer = csv.DictWriter(f, fieldnames=list(log_rows[0].keys()))\n",
        "                writer.writeheader()\n",
        "                writer.writerows(log_rows)\n",
        "\n",
        "            print(f\"✓ Checkpoint saved at episode {episode+1}\")\n",
        "\n",
        "    # Save final model\n",
        "    final_model_path = 'yahtzee_model_final.pth'\n",
        "    agent.save(final_model_path)\n",
        "\n",
        "    # Write final CSV\n",
        "    with open(csv_path, 'w', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=list(log_rows[0].keys()))\n",
        "        writer.writeheader()\n",
        "        writer.writerows(log_rows)\n",
        "\n",
        "    print(f\"\\n✓ Training complete!\")\n",
        "    print(f\"✓ CSV saved to {csv_path}\")\n",
        "    print(f\"✓ Final model saved to {final_model_path}\\n\")\n",
        "\n",
        "    return agent\n",
        "\n",
        "\n",
        "# Usage example:\n",
        "# Resume from episode 43000 and continue to 50000\n",
        "agent = resume_training(\n",
        "    checkpoint_path=checkpoint_path,  # '/content/drive/MyDrive/Yathzee/checkpoint.pth'\n",
        "    csv_path='training_log.csv',\n",
        "    total_episodes=50000,\n",
        "    checkpoint_interval=1000\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKld1ywaQedu"
      },
      "source": [
        "## Test the Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvShJTG1Qedu",
        "outputId": "96478e93-d5ff-4c66-d38f-804f376e91b9"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "test_scores = test_agent(agent, num_games=100, verbose=False)\n",
        "\n",
        "# Save training results for external plotting\n",
        "with open(\"test_scores.csv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"game\", \"score\"])\n",
        "    for i, s in enumerate(test_scores):\n",
        "        writer.writerow([i, s])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
